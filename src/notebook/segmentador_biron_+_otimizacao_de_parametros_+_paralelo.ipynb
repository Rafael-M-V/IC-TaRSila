{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46z-4Bm6e-5M"
      },
      "source": [
        "## Comandos para compactar arquivos do drive para depois baixar e descompactar\n",
        "\n",
        "### (usar novamente caso arquivos das pastas automatic-segmentation ou do inquérito em questão forem modificados)\n",
        "\n",
        "* !cd /content/drive/MyDrive/; tar -zcvf SP_D2_255_segmentado.tar.gz SP_D2_255_segmentado\n",
        "\n",
        "* !cd /content/drive/MyDrive/; tar -zcvf automatic-segmentation.tar.gz automatic-segmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L954IqH0qQIj",
        "outputId": "18fe7eb5-0b80-4552-af0b-3c37c2a1e033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-EpqeWanszqcpLTBFYXXdXHlrC7HT0Aj\n",
            "To: /content/automatic-segmentation.tar.gz\n",
            "100% 215M/215M [00:01<00:00, 207MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1-EpqeWanszqcpLTBFYXXdXHlrC7HT0Aj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFEkKkubq7uP",
        "outputId": "692eeb1f-4a16-4138-bb3a-65d337079f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "automatic-segmentation/\n",
            "automatic-segmentation/NURCSPTranscriptConverter.py\n",
            "automatic-segmentation/pom.xml\n",
            "automatic-segmentation/requirements.txt\n",
            "automatic-segmentation/GramaticalClass.py\n",
            "automatic-segmentation/server.py\n",
            "automatic-segmentation/Procfile\n",
            "automatic-segmentation/manifest.json\n",
            "automatic-segmentation/Parser.py\n",
            "automatic-segmentation/phoneme_rules.py\n",
            "automatic-segmentation/recog.out\n",
            "automatic-segmentation/NURCSP_convert_transcripts.py\n",
            "automatic-segmentation/erros.txt\n",
            "automatic-segmentation/LICENSE\n",
            "automatic-segmentation/README.md\n",
            "automatic-segmentation/Segmentador.py\n",
            "automatic-segmentation/segments.csv\n",
            "automatic-segmentation/corpus/\n",
            "automatic-segmentation/htk/\n",
            "automatic-segmentation/htk/README\n",
            "automatic-segmentation/htk/LICENSE\n",
            "automatic-segmentation/htk/HLMTools/\n",
            "automatic-segmentation/htk/HLMTools/LNewMap.c\n",
            "automatic-segmentation/htk/HLMTools/LPlex.c\n",
            "automatic-segmentation/htk/HLMTools/LMerge.c\n",
            "automatic-segmentation/htk/HLMTools/LNorm.c\n",
            "automatic-segmentation/htk/HLMTools/LSubset.c\n",
            "automatic-segmentation/htk/HLMTools/LGCopy.c\n",
            "automatic-segmentation/htk/HLMTools/LLink.c\n",
            "automatic-segmentation/htk/HLMTools/LFoF.c\n",
            "automatic-segmentation/htk/HLMTools/MakefileMKL\n",
            "automatic-segmentation/htk/HLMTools/LGList.c\n",
            "automatic-segmentation/htk/HLMTools/MakefileCPU\n",
            "automatic-segmentation/htk/HLMTools/HLMCopy.c\n",
            "automatic-segmentation/htk/HLMTools/LGPrep.c\n",
            "automatic-segmentation/htk/HLMTools/LAdapt.c\n",
            "automatic-segmentation/htk/HLMTools/MakefileNVCC\n",
            "automatic-segmentation/htk/HLMTools/LBuild.c\n",
            "automatic-segmentation/htk/HLMTools/Cluster.c\n",
            "automatic-segmentation/htk/HLMLib/\n",
            "automatic-segmentation/htk/HLMLib/LWMap.h\n",
            "automatic-segmentation/htk/HLMLib/MakefileNVCC\n",
            "automatic-segmentation/htk/HLMLib/LCMap.h\n",
            "automatic-segmentation/htk/HLMLib/LGBase.c\n",
            "automatic-segmentation/htk/HLMLib/LModel.c\n",
            "automatic-segmentation/htk/HLMLib/LPMerge.c\n",
            "automatic-segmentation/htk/HLMLib/MakefileCPU\n",
            "automatic-segmentation/htk/HLMLib/LWMap.c\n",
            "automatic-segmentation/htk/HLMLib/LPCalc.h\n",
            "automatic-segmentation/htk/HLMLib/LPMerge.h\n",
            "automatic-segmentation/htk/HLMLib/LUtil.c\n",
            "automatic-segmentation/htk/HLMLib/LGBase.h\n",
            "automatic-segmentation/htk/HLMLib/LPCalc.c\n",
            "automatic-segmentation/htk/HLMLib/LModel.h\n",
            "automatic-segmentation/htk/HLMLib/MakefileMKL\n",
            "automatic-segmentation/htk/HLMLib/LCMap.c\n",
            "automatic-segmentation/htk/HLMLib/LUtil.h\n",
            "automatic-segmentation/htk/HTKLib/\n",
            "automatic-segmentation/htk/HTKLib/HGraf.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HGraf.o\n",
            "automatic-segmentation/htk/HTKLib/HDict.c\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.c\n",
            "automatic-segmentation/htk/HTKLib/esig_asc.o\n",
            "automatic-segmentation/htk/HTKLib/esig_nat.c\n",
            "automatic-segmentation/htk/HTKLib/HNCache.h\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.o\n",
            "automatic-segmentation/htk/HTKLib/HParm.c\n",
            "automatic-segmentation/htk/HTKLib/HArc.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HShell.o\n",
            "automatic-segmentation/htk/HTKLib/HRec.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HParm.h\n",
            "automatic-segmentation/htk/HTKLib/HMath.o\n",
            "automatic-segmentation/htk/HTKLib/esig_edr.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HTrain.o\n",
            "automatic-segmentation/htk/HTKLib/HLat.h\n",
            "automatic-segmentation/htk/HTKLib/HLabel.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HDict.o\n",
            "automatic-segmentation/htk/HTKLib/HMem.o\n",
            "automatic-segmentation/htk/HTKLib/HLabel.o\n",
            "automatic-segmentation/htk/HTKLib/HSigP.h\n",
            "automatic-segmentation/htk/HTKLib/esig_edr.o\n",
            "automatic-segmentation/htk/HTKLib/HLabel.c\n",
            "automatic-segmentation/htk/HTKLib/esignal.o\n",
            "automatic-segmentation/htk/HTKLib/HModel.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HTKLiblv.a\n",
            "automatic-segmentation/htk/HTKLib/HVQ.h\n",
            "automatic-segmentation/htk/HTKLib/HShell.h\n",
            "automatic-segmentation/htk/HTKLib/HMap.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HLabel.h\n",
            "automatic-segmentation/htk/HTKLib/HUtil.c\n",
            "automatic-segmentation/htk/HTKLib/esig_nat.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HArc.c\n",
            "automatic-segmentation/htk/HTKLib/HMem.lv.o\n",
            "automatic-segmentation/htk/HTKLib/config.h\n",
            "automatic-segmentation/htk/HTKLib/HFB.h\n",
            "automatic-segmentation/htk/HTKLib/HNCache.c\n",
            "automatic-segmentation/htk/HTKLib/HFB.c\n",
            "automatic-segmentation/htk/HTKLib/HTrain.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HCUDA.cu\n",
            "automatic-segmentation/htk/HTKLib/HSigP.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HWave.h\n",
            "automatic-segmentation/htk/HTKLib/HLat.c\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.c\n",
            "automatic-segmentation/htk/HTKLib/HNet.c\n",
            "automatic-segmentation/htk/HTKLib/HUtil.h\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.o\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.h\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HUtil.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HLM.h\n",
            "automatic-segmentation/htk/HTKLib/HRec.h\n",
            "automatic-segmentation/htk/HTKLib/esig_nat.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.c\n",
            "automatic-segmentation/htk/HTKLib/HTKLib.a\n",
            "automatic-segmentation/htk/HTKLib/HMath.h\n",
            "automatic-segmentation/htk/HTKLib/esig_asc.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HMem.c\n",
            "automatic-segmentation/htk/HTKLib/HParm.o\n",
            "automatic-segmentation/htk/HTKLib/esignal.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HShell.c\n",
            "automatic-segmentation/htk/HTKLib/HRec.o\n",
            "automatic-segmentation/htk/HTKLib/HFBLat.h\n",
            "automatic-segmentation/htk/HTKLib/HModel.h\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.lv.o\n",
            "automatic-segmentation/htk/HTKLib/esig_edr.c\n",
            "automatic-segmentation/htk/HTKLib/HVQ.o\n",
            "automatic-segmentation/htk/HTKLib/HLat.o\n",
            "automatic-segmentation/htk/HTKLib/HWave.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.o\n",
            "automatic-segmentation/htk/HTKLib/HGraf.c\n",
            "automatic-segmentation/htk/HTKLib/HANNet.c\n",
            "automatic-segmentation/htk/HTKLib/HModel.o\n",
            "automatic-segmentation/htk/HTKLib/HDict.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HExactMPE.h\n",
            "automatic-segmentation/htk/HTKLib/HDict.h\n",
            "automatic-segmentation/htk/HTKLib/HMem.h\n",
            "automatic-segmentation/htk/HTKLib/HMap.o\n",
            "automatic-segmentation/htk/HTKLib/esignal.c\n",
            "automatic-segmentation/htk/HTKLib/HGraf.null.c\n",
            "automatic-segmentation/htk/HTKLib/HUtil.o\n",
            "automatic-segmentation/htk/HTKLib/HANNet.o\n",
            "automatic-segmentation/htk/HTKLib/HAudio.h\n",
            "automatic-segmentation/htk/HTKLib/HCUDA.h\n",
            "automatic-segmentation/htk/HTKLib/HShell.lv.o\n",
            "automatic-segmentation/htk/HTKLib/MakefileMKL\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.c\n",
            "automatic-segmentation/htk/HTKLib/HNet.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HRec.c\n",
            "automatic-segmentation/htk/HTKLib/HWave.o\n",
            "automatic-segmentation/htk/HTKLib/esig_asc.c\n",
            "automatic-segmentation/htk/HTKLib/HANNet.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HArc.h\n",
            "automatic-segmentation/htk/HTKLib/HMap.h\n",
            "automatic-segmentation/htk/HTKLib/HNet.h\n",
            "automatic-segmentation/htk/HTKLib/HLM.c\n",
            "automatic-segmentation/htk/HTKLib/HTrain.h\n",
            "automatic-segmentation/htk/HTKLib/HSigP.c\n",
            "automatic-segmentation/htk/HTKLib/HANNet.h\n",
            "automatic-segmentation/htk/HTKLib/HLM.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HVQ.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HMap.c\n",
            "automatic-segmentation/htk/HTKLib/HLat.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HGraf.h\n",
            "automatic-segmentation/htk/HTKLib/HWave.c\n",
            "automatic-segmentation/htk/HTKLib/MakefileNVCC\n",
            "automatic-segmentation/htk/HTKLib/MakefileCPU\n",
            "automatic-segmentation/htk/HTKLib/HNet.o\n",
            "automatic-segmentation/htk/HTKLib/HFB.o\n",
            "automatic-segmentation/htk/HTKLib/HArc.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HLM.o\n",
            "automatic-segmentation/htk/HTKLib/HNCache.o\n",
            "automatic-segmentation/htk/HTKLib/HModel.c\n",
            "automatic-segmentation/htk/HTKLib/HSigP.o\n",
            "automatic-segmentation/htk/HTKLib/HTrain.c\n",
            "automatic-segmentation/htk/HTKLib/HVQ.c\n",
            "automatic-segmentation/htk/HTKLib/HMath.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HMath.c\n",
            "automatic-segmentation/htk/HTKLib/HAdapt.o\n",
            "automatic-segmentation/htk/HTKLib/HNCache.lv.o\n",
            "automatic-segmentation/htk/HTKLib/HParm.lv.o\n",
            "automatic-segmentation/htk/HTKLib/esignal.h\n",
            "automatic-segmentation/htk/HTKLib/HFB.lv.o\n",
            "automatic-segmentation/htk/HTKLib/lib/\n",
            "automatic-segmentation/htk/HTKLib/lib/HTKLiblv.a\n",
            "automatic-segmentation/htk/HTKLib/lib/HTKLib.a\n",
            "automatic-segmentation/htk/samples/\n",
            "automatic-segmentation/htk/samples/LICENSE\n",
            "automatic-segmentation/htk/samples/LMTutorial/\n",
            "automatic-segmentation/htk/samples/LMTutorial/5k.wlist\n",
            "automatic-segmentation/htk/samples/LMTutorial/config\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/LCond.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/60k.wlist\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/getwordlist.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/intersection.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/extras/TagTextWithClassMap.pl\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/scandal_in_bohemia.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/threeudents.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/his_last_bow.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/wisteria_lodge.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/dying_detective.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/reigate_puzzle.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/resident_patient.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/final_problem.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/stock_brokers_clerk.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/red_circle.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/yellow_face.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/secondain.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/black_peter.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/solitary_cyclist.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/abbey_grange.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/six_napoleons.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/empty_house.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/dancing_men.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/case_of_identity.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/silver_blaze.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/hound_of_baskervilles.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/valley_of_fear.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/crooked_man.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/study_in_scarlet.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/devils_foot.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/greek_interpreter.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/golden_pince-nez.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/lady_frances_carfax.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/blue_carbuncle.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/sign_of_four.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/gloria_scott.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/copper_beeches.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/cardboard_box.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/engineers_thumb.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/priory_school.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/charles_agustus_milverton.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/preface.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/bruce-partington_plans.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/norwood_builder.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/man_with_twisted_lip.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/beryl_coronet.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/speckled_band.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/musgrave_ritual.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/five_orange_pips.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/noble_bachelor.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/boscombe_valley.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/missing_three-quarter.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/train/naval_treaty.txt\n",
            "automatic-segmentation/htk/samples/LMTutorial/test/\n",
            "automatic-segmentation/htk/samples/LMTutorial/test/red-headed_league.txt\n",
            "automatic-segmentation/htk/samples/RMHTK/\n",
            "automatic-segmentation/htk/samples/RMHTK/environment\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-align\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/w_decode\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/coderm\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/hbuild\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/finetune\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/sequence\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-makelm\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/mkclscript\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/addmlp\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-decode\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/w_edit\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/latgen-dnn\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/hedit\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/htestrm\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/forward\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/pretrain\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/herest\n",
            "automatic-segmentation/htk/samples/RMHTK/scripts/fixrm\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/MakeProtoHMMSet.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/rest_discrete.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/hedit.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/env_conv.pm\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/latgen.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mk_sub_list.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mk_mlf.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/discreteMonoSet.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/full_list.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/hbuild.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/coderm.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/ng_net.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/herest.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/init_discrete.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/wp_net.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/global.pl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/fixrm.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/get_ndx.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mkclscript.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/mk_lab.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/htestrm.prl\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/Carp.pm\n",
            "automatic-segmentation/htk/samples/RMHTK/perl_scripts/Exporter.pm\n",
            "automatic-segmentation/htk/samples/RMHTK/work/\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/README\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/varProto\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/HTE\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/hmm0/\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/hmm0/MODELS\n",
            "automatic-segmentation/htk/samples/RMHTK/work/R1/hmm0/MODELS~\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/GenInitDNN.py\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/ConvertExtSCP.py\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/CatHTKFeatures.py\n",
            "automatic-segmentation/htk/samples/RMHTK/python_scripts/SubsetSCP.py\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/regtree_c2.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/sents.snr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/replacesil.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/eq.ng\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/maketandem.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/corrupt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/discrete.hmm\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/wordlist\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/discrete.sp\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/quests.hed\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/null.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/eq.wp\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/xwrd.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/mono.dct\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/tri.ded\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/pcdsril.txt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/sil.txt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/mono.ded\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/mono.hd.dct\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/dicts/tri.dct\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/xwrd.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/mono.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/labs/tri.hled\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/full.list.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/ng.net.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/wp.net.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/awks/mlflabs2scp.awk\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.adapt\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlr.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mlp.uncompress\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.discrete\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnncvn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnn.hd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlm\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.cmllr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.basic\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.debug\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mpe\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hd.mod\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnnbasic\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.xwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlm.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.dnn.xwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlda\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hd.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.compress\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.code\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.xwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mmi\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mpe.win\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mllr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.mfc.uncompress\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.hlr\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.tandem.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.basic\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.logxwrd\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/configs/config.cov\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/info/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/info/ident_MFCC_E_D_A_Z_cvn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.decode\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.hlda\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.am\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.align\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.mpe\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.tandem.semit\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.sat.xform\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.align.trn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.sat.model\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.latgen\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.mpe\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.latgen\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.dnn.bn\n",
            "automatic-segmentation/htk/samples/RMHTK/lib/htefiles/HTE.latgen.3\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-17\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-5\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-4\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-15\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-0\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-10\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-11\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-3\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-1\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-9\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-12\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-2\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-13\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-7\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-8\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-16\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-14\n",
            "automatic-segmentation/htk/samples/RMHTK/steps/step-6\n",
            "automatic-segmentation/htk/samples/HTKDemo/\n",
            "automatic-segmentation/htk/samples/HTKDemo/test.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/ChangeLog\n",
            "automatic-segmentation/htk/samples/HTKDemo/runDemo.pl\n",
            "automatic-segmentation/htk/samples/HTKDemo/README.NT\n",
            "automatic-segmentation/htk/samples/HTKDemo/MakeProtoHMMSet\n",
            "automatic-segmentation/htk/samples/HTKDemo/README\n",
            "automatic-segmentation/htk/samples/HTKDemo/train.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/monlabs.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/runDemo\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/outaudio\n",
            "automatic-segmentation/htk/samples/HTKDemo/accs.scr\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triTiedStateS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S3Tree.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triPlainS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/rbiPlainS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monSharedM1S3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S3FullCov.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S1Tree.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM4S1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triDiscM64S3HSmooth.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monTiedMixS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triSharedS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triDiscM64S3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triTiedMixS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/lbiPlainS1.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/DcfFormat\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S3HERestPell.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S1Lin.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monDiscM64S3Lin.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/triTiedMixS1HSmooth.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/directAudio.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monTiedMixS3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/configs/monPlainM1S3.dcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triTiedStateS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triDiscM64S3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S1Lin.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triTiedMixS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triPlainS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/rbiPlainS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monSharedM1S3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S3FullCov.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S3Lin.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triTiedMixS1HSmooth.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triDiscM64S3HSmooth.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM4S1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/lbiPlainS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/triSharedS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S1Tree.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monPlainM1S3HERestPell.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monTiedMixS3.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monDiscM64S3Tree.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/results/monTiedMixS1.res\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monInternalTie.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiTiedState.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edllabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/contDepPlainhs.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edrlabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monInternalTie(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edtlabs(1).led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edllabs(1).led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiTiedState(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM1(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edrlabs(1).led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edlabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/contDepTiedhsS3.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/contDepTiedhsS1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM4(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/rbiTiedState.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/rbiSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS3(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/edtlabs.led\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/lbiSharedhsM1(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS1(1).hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/triSharedhsM4.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monTiedhsS3.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/triTiedState.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/monSharedhsM4.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/edfiles/triSharedhsM1.hed\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/mon/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/lbi/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/rbi/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr5.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/te2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr6.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr2.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr7.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/te1.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/te3.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/labels/bcplabs/tri/tr4.lab\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/triNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/rbiLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/triLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/rbiNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/monLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/monNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/lbiLattice\n",
            "automatic-segmentation/htk/samples/HTKDemo/networks/lbiNetwork\n",
            "automatic-segmentation/htk/samples/HTKDemo/codebooks/\n",
            "automatic-segmentation/htk/samples/HTKDemo/codebooks/currentCodebook\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr7.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr3.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te3.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te3.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr4.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr3.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr4.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr1.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr6.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr5.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr7.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr6.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te2.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr1.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr5.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te1.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te1.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/te2.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr2.phn\n",
            "automatic-segmentation/htk/samples/HTKDemo/tidata/tr2.adc\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herest_cmn.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hquant.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hinit.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hsmoothCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hhed.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hvite_cmn.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hrestVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hrestCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hcopy.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hsmoothVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hviteDA.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herestVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/config.audio\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herestCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hviteVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/herest.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hviteCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hsmooth.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hvite.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hrest.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hinitCD.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hcopyDA.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hinitVQ.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/toolconfs/hcopyFB.conf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/example.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m64_vq.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s3_m1_1_1_fc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m12_dc_tied.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m4_dc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s3_m64_64_16_vq.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m4_fc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m1_dc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s3_m1_1_1_dc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/protoconfs/proto_s1_m1_fc.pcf\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/contDepList\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/lbiVocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/triVocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/dataList3\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/dataList1\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/trainFileStubs\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/allPairedFiles\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/rbiVocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/bcplist\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/testFileStubs\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/allTrainFiles\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/dataList2\n",
            "automatic-segmentation/htk/samples/HTKDemo/lists/bcpvocab\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/te2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/te1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/test/te3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/te3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/te2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr6.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr7.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/te1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr4.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/store/tr5.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr2.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr1.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr3.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr4.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr7.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr5.mfc\n",
            "automatic-segmentation/htk/samples/HTKDemo/data/train/tr6.mfc\n",
            "automatic-segmentation/htk/samples/HTKTutorial/\n",
            "automatic-segmentation/htk/samples/HTKTutorial/prompts2mlf\n",
            "automatic-segmentation/htk/samples/HTKTutorial/prompts2wlist\n",
            "automatic-segmentation/htk/samples/HTKTutorial/makesp\n",
            "automatic-segmentation/htk/samples/HTKTutorial/ChangeLog\n",
            "automatic-segmentation/htk/samples/HTKTutorial/maketrihed\n",
            "automatic-segmentation/htk/HTKTools/\n",
            "automatic-segmentation/htk/HTKTools/HMMIRest\n",
            "automatic-segmentation/htk/HTKTools/MakefileNVCC\n",
            "automatic-segmentation/htk/HTKTools/HResults.c\n",
            "automatic-segmentation/htk/HTKTools/HMMIRest.c\n",
            "automatic-segmentation/htk/HTKTools/HNForward.c\n",
            "automatic-segmentation/htk/HTKTools/HLEd\n",
            "automatic-segmentation/htk/HTKTools/HSmooth\n",
            "automatic-segmentation/htk/HTKTools/HCompV.c\n",
            "automatic-segmentation/htk/HTKTools/HQuant\n",
            "automatic-segmentation/htk/HTKTools/HInit.c\n",
            "automatic-segmentation/htk/HTKTools/HBuild.c\n",
            "automatic-segmentation/htk/HTKTools/HList\n",
            "automatic-segmentation/htk/HTKTools/HSGen.c\n",
            "automatic-segmentation/htk/HTKTools/HLEd.c\n",
            "automatic-segmentation/htk/HTKTools/HBuild\n",
            "automatic-segmentation/htk/HTKTools/HRest\n",
            "automatic-segmentation/htk/HTKTools/HVite.c\n",
            "automatic-segmentation/htk/HTKTools/HParse\n",
            "automatic-segmentation/htk/HTKTools/HNTrainSGD\n",
            "automatic-segmentation/htk/HTKTools/HCompV\n",
            "automatic-segmentation/htk/HTKTools/HLRescore\n",
            "automatic-segmentation/htk/HTKTools/HCopy\n",
            "automatic-segmentation/htk/HTKTools/HHEd.c\n",
            "automatic-segmentation/htk/HTKTools/HLConf\n",
            "automatic-segmentation/htk/HTKTools/HResults\n",
            "automatic-segmentation/htk/HTKTools/HNTrainSGD.c\n",
            "automatic-segmentation/htk/HTKTools/HSLab.c\n",
            "automatic-segmentation/htk/HTKTools/HDMan\n",
            "automatic-segmentation/htk/HTKTools/HSGen\n",
            "automatic-segmentation/htk/HTKTools/HParse.c\n",
            "automatic-segmentation/htk/HTKTools/HLStats\n",
            "automatic-segmentation/htk/HTKTools/HCopy.c\n",
            "automatic-segmentation/htk/HTKTools/HVite\n",
            "automatic-segmentation/htk/HTKTools/HERest\n",
            "automatic-segmentation/htk/HTKTools/HNForward\n",
            "automatic-segmentation/htk/HTKTools/HQuant.c\n",
            "automatic-segmentation/htk/HTKTools/HRest.c\n",
            "automatic-segmentation/htk/HTKTools/HSmooth.c\n",
            "automatic-segmentation/htk/HTKTools/HList.c\n",
            "automatic-segmentation/htk/HTKTools/HLRescore.c\n",
            "automatic-segmentation/htk/HTKTools/HInit\n",
            "automatic-segmentation/htk/HTKTools/HLStats.c\n",
            "automatic-segmentation/htk/HTKTools/MakefileMKL\n",
            "automatic-segmentation/htk/HTKTools/HLConf.c\n",
            "automatic-segmentation/htk/HTKTools/HDMan.c\n",
            "automatic-segmentation/htk/HTKTools/HHEd\n",
            "automatic-segmentation/htk/HTKTools/HERest.c\n",
            "automatic-segmentation/htk/HTKTools/MakefileCPU\n",
            "automatic-segmentation/htk/bin.win32/\n",
            "automatic-segmentation/htk/bin.win32/HRest.exe\n",
            "automatic-segmentation/htk/bin.win32/Cluster.exe\n",
            "automatic-segmentation/htk/bin.win32/HHEd.exe\n",
            "automatic-segmentation/htk/bin.win32/LGList.exe\n",
            "automatic-segmentation/htk/bin.win32/HResults.exe\n",
            "automatic-segmentation/htk/bin.win32/LBuild.exe\n",
            "automatic-segmentation/htk/bin.win32/LNewMap.exe\n",
            "automatic-segmentation/htk/bin.win32/HSGen.exe\n",
            "automatic-segmentation/htk/bin.win32/LSubset.exe\n",
            "automatic-segmentation/htk/bin.win32/HCompV.exe\n",
            "automatic-segmentation/htk/bin.win32/LGPrep.exe\n",
            "automatic-segmentation/htk/bin.win32/HInit.exe\n",
            "automatic-segmentation/htk/bin.win32/LLink.exe\n",
            "automatic-segmentation/htk/bin.win32/HERest.exe\n",
            "automatic-segmentation/htk/bin.win32/LPlex.exe\n",
            "automatic-segmentation/htk/bin.win32/HSmooth.exe\n",
            "automatic-segmentation/htk/bin.win32/HVite.exe\n",
            "automatic-segmentation/htk/bin.win32/HLRescore.exe\n",
            "automatic-segmentation/htk/bin.win32/HParse.exe\n",
            "automatic-segmentation/htk/bin.win32/HList.exe\n",
            "automatic-segmentation/htk/bin.win32/LFoF.exe\n",
            "automatic-segmentation/htk/bin.win32/LMerge.exe\n",
            "automatic-segmentation/htk/bin.win32/HLEd.exe\n",
            "automatic-segmentation/htk/bin.win32/HBuild.exe\n",
            "automatic-segmentation/htk/bin.win32/HLStats.exe\n",
            "automatic-segmentation/htk/bin.win32/HMMIRest.exe\n",
            "automatic-segmentation/htk/bin.win32/HSLab.exe\n",
            "automatic-segmentation/htk/bin.win32/HQuant.exe\n",
            "automatic-segmentation/htk/bin.win32/HDMan.exe\n",
            "automatic-segmentation/htk/bin.win32/LGCopy.exe\n",
            "automatic-segmentation/htk/bin.win32/HLMCopy.exe\n",
            "automatic-segmentation/htk/bin.win32/LAdapt.exe\n",
            "automatic-segmentation/htk/bin.win32/LNorm.exe\n",
            "automatic-segmentation/htk/bin.win32/HCopy.exe\n",
            "automatic-segmentation/htk/bin.cpu/\n",
            "automatic-segmentation/htk/bin.cpu/HResults\n",
            "automatic-segmentation/htk/bin.cpu/HRest\n",
            "automatic-segmentation/htk/bin.cpu/HVite\n",
            "automatic-segmentation/htk/bin.cpu/HSGen\n",
            "automatic-segmentation/htk/bin.cpu/HERest\n",
            "automatic-segmentation/htk/bin.cpu/HHEd\n",
            "automatic-segmentation/htk/bin.cpu/HCopy\n",
            "automatic-segmentation/htk/bin.cpu/HParse\n",
            "automatic-segmentation/htk/bin.cpu/HNTrainSGD\n",
            "automatic-segmentation/htk/bin.cpu/HLRescore\n",
            "automatic-segmentation/htk/bin.cpu/HInit\n",
            "automatic-segmentation/htk/bin.cpu/HDMan\n",
            "automatic-segmentation/htk/bin.cpu/HSmooth\n",
            "automatic-segmentation/htk/bin.cpu/HNForward\n",
            "automatic-segmentation/htk/bin.cpu/HQuant\n",
            "automatic-segmentation/htk/bin.cpu/HLEd\n",
            "automatic-segmentation/htk/bin.cpu/HLConf\n",
            "automatic-segmentation/htk/bin.cpu/HMMIRest\n",
            "automatic-segmentation/htk/bin.cpu/HLStats\n",
            "automatic-segmentation/htk/bin.cpu/HCompV\n",
            "automatic-segmentation/htk/bin.cpu/HBuild\n",
            "automatic-segmentation/htk/bin.cpu/HList\n",
            "automatic-segmentation/bin/\n",
            "automatic-segmentation/bin/mfa_generate_dictionary\n",
            "automatic-segmentation/bin/mfa_align\n",
            "automatic-segmentation/bin/mfa_train_and_align\n",
            "automatic-segmentation/bin/mfa_train_g2p\n",
            "automatic-segmentation/__pycache__/\n",
            "automatic-segmentation/__pycache__/Word.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/hash_map.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/phoneme_rules.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/GramaticalClass.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/Parser.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/TextConverter.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/NURCSPTranscriptConverter.cpython-37.pyc\n",
            "automatic-segmentation/__pycache__/Conversor.cpython-37.pyc\n",
            "automatic-segmentation/tmp/\n",
            "automatic-segmentation/tmp/dict\n",
            "automatic-segmentation/tmp/mlf.mlf\n",
            "automatic-segmentation/tmp/mfc.matl\n",
            "automatic-segmentation/tmp/audios.scp\n",
            "automatic-segmentation/tmp/recog.out\n",
            "automatic-segmentation/templates/\n",
            "automatic-segmentation/templates/how.html\n",
            "automatic-segmentation/templates/alinhador.html\n",
            "automatic-segmentation/templates/add.html\n",
            "automatic-segmentation/templates/about.html\n",
            "automatic-segmentation/templates/kaldi.html\n",
            "automatic-segmentation/templates/index.html\n",
            "automatic-segmentation/templates/meta.html\n",
            "automatic-segmentation/lib/\n",
            "automatic-segmentation/lib/_sha256.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_iso2022.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/binascii.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/select.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/audioop.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libpython3.6m.so.1.0\n",
            "automatic-segmentation/lib/_pickle.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/fcntl.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libncursesw.so.6\n",
            "automatic-segmentation/lib/_codecs_tw.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_ssl.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_bz2.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/resource.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/array.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/pyexpat.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_bisect.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_lzma.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libtinfow.so.6\n",
            "automatic-segmentation/lib/libz.so.1\n",
            "automatic-segmentation/lib/libcrypto.so.1.0.0\n",
            "automatic-segmentation/lib/math.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_datetime.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_socket.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/generate_dictionary\n",
            "automatic-segmentation/lib/_posixsubprocess.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_heapq.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_opcode.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/termios.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_ctypes.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_kr.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_sha1.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/config\n",
            "automatic-segmentation/lib/align\n",
            "automatic-segmentation/lib/_random.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_struct.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libpython3.6m.so\n",
            "automatic-segmentation/lib/_codecs_cn.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_hk.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libffi.so.6\n",
            "automatic-segmentation/lib/readline.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_sha512.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/mmap.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_multibytecodec.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_codecs_jp.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/grp.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/hmm_names\n",
            "automatic-segmentation/lib/_blake2.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/libreadline.so.7\n",
            "automatic-segmentation/lib/_multiprocessing.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/config.parming\n",
            "automatic-segmentation/lib/libssl.so.1.0.0\n",
            "automatic-segmentation/lib/_decimal.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_hashlib.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_md5.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/base_library.zip\n",
            "automatic-segmentation/lib/grammar\n",
            "automatic-segmentation/lib/_sha3.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/liblzma.so.5\n",
            "automatic-segmentation/lib/train_and_align\n",
            "automatic-segmentation/lib/unicodedata.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/train_g2p\n",
            "automatic-segmentation/lib/zlib.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/_struct/\n",
            "automatic-segmentation/lib/_struct/cpython-36m-x86_64-linux-gnu/\n",
            "automatic-segmentation/lib/_struct/cpython-36m-x86_64-linux-gnu/sotruct.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/zlib/\n",
            "automatic-segmentation/lib/zlib/cpython-36m-x86_64-linux-gnu/\n",
            "automatic-segmentation/lib/zlib/cpython-36m-x86_64-linux-gnu/soib.cpython-36m-x86_64-linux-gnu.so\n",
            "automatic-segmentation/lib/include/\n",
            "automatic-segmentation/lib/include/python3.6m/\n",
            "automatic-segmentation/lib/include/python3.6m/pyconfig.h\n",
            "automatic-segmentation/lib/lib/\n",
            "automatic-segmentation/lib/lib/python3.6/\n",
            "automatic-segmentation/lib/lib/python3.6/config-3.6m-x86_64-linux-gnu/\n",
            "automatic-segmentation/lib/lib/python3.6/config-3.6m-x86_64-linux-gnu/Makefile\n",
            "automatic-segmentation/lib/thirdparty/\n",
            "automatic-segmentation/lib/thirdparty/bin/\n",
            "automatic-segmentation/lib/thirdparty/bin/acc-tree-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-base.so\n",
            "automatic-segmentation/lib/thirdparty/bin/feat-to-len\n",
            "automatic-segmentation/lib/thirdparty/bin/build-tree\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-acc-stats-ali\n",
            "automatic-segmentation/lib/thirdparty/bin/farcompilestrings\n",
            "automatic-segmentation/lib/thirdparty/bin/fstcopy\n",
            "automatic-segmentation/lib/thirdparty/bin/ali-to-pdf\n",
            "automatic-segmentation/lib/thirdparty/bin/lattice-align-words\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-latgen-faster\n",
            "automatic-segmentation/lib/thirdparty/bin/compile-questions\n",
            "automatic-segmentation/lib/thirdparty/bin/ali-to-post\n",
            "automatic-segmentation/lib/thirdparty/bin/libfstfar.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/libfst.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-sum-accs\n",
            "automatic-segmentation/lib/thirdparty/bin/est-lda\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-acc-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/est-mllt\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-est\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-sum-accs\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-acc-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-info\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-gselect\n",
            "automatic-segmentation/lib/thirdparty/bin/convert-ali\n",
            "automatic-segmentation/lib/thirdparty/bin/feat-to-dim\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-decoder.so\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-hmm.so\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-ivector.so\n",
            "automatic-segmentation/lib/thirdparty/bin/fstcompile\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-align-compiled\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-est-fmllr\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-gmm.so\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extract\n",
            "automatic-segmentation/lib/thirdparty/bin/extract-segments\n",
            "automatic-segmentation/lib/thirdparty/bin/apply-cmvn\n",
            "automatic-segmentation/lib/thirdparty/bin/libfstfarscript.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-est\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-init-model\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-cudamatrix.so\n",
            "automatic-segmentation/lib/thirdparty/bin/compute-cmvn-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-init-from-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/compute-mfcc-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/acc-lda\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-transform-means\n",
            "automatic-segmentation/lib/thirdparty/bin/compile-train-graphs\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-to-fgmm\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-get-post\n",
            "automatic-segmentation/lib/thirdparty/bin/draw-tree\n",
            "automatic-segmentation/lib/thirdparty/bin/copy-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/cluster-phones\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-global-est\n",
            "automatic-segmentation/lib/thirdparty/bin/align-equal-compiled\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-init\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-init-mono\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-mixup\n",
            "automatic-segmentation/lib/thirdparty/bin/fstarcsort\n",
            "automatic-segmentation/lib/thirdparty/bin/lattice-to-phone-lattice\n",
            "automatic-segmentation/lib/thirdparty/bin/fstdraw\n",
            "automatic-segmentation/lib/thirdparty/bin/compile-train-graphs-fsts\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-boost-silence\n",
            "automatic-segmentation/lib/thirdparty/bin/lattice-oracle\n",
            "automatic-segmentation/lib/thirdparty/bin/ivector-extractor-sum-accs\n",
            "automatic-segmentation/lib/thirdparty/bin/gmm-acc-mllt\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-feat.so\n",
            "automatic-segmentation/lib/thirdparty/bin/add-deltas\n",
            "automatic-segmentation/lib/thirdparty/bin/libfstscript.so.13\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-fstext.so\n",
            "automatic-segmentation/lib/thirdparty/bin/append-vector-to-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/compose-transforms\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-nnet2.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-adjust-priors\n",
            "automatic-segmentation/lib/thirdparty/bin/ngramsymbols\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-to-raw-nnet\n",
            "automatic-segmentation/lib/thirdparty/bin/libopenblas.so.0\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-get-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-align-compiled\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-init\n",
            "automatic-segmentation/lib/thirdparty/bin/matrix-sum-rows\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-insert\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-matrix.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-info\n",
            "automatic-segmentation/lib/thirdparty/bin/libngramhist.so.134\n",
            "automatic-segmentation/lib/thirdparty/bin/ngrammake\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-get-feature-transform\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-util.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-compute-from-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-lat.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-compute-prob\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-init\n",
            "automatic-segmentation/lib/thirdparty/bin/linear-to-nbest\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-relabel-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-tree.so\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-mixup\n",
            "automatic-segmentation/lib/thirdparty/bin/ngramprint\n",
            "automatic-segmentation/lib/thirdparty/bin/nbest-to-ctm\n",
            "automatic-segmentation/lib/thirdparty/bin/libngram.so.134\n",
            "automatic-segmentation/lib/thirdparty/bin/ngramcount\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-average\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-subset-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-am-copy\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-copy-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-shuffle-egs\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-transform.so\n",
            "automatic-segmentation/lib/thirdparty/bin/libkaldi-lm.so\n",
            "automatic-segmentation/lib/thirdparty/bin/show-transitions\n",
            "automatic-segmentation/lib/thirdparty/bin/paste-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-train-transitions\n",
            "automatic-segmentation/lib/thirdparty/bin/splice-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/phonetisaurus-arpa2wfst\n",
            "automatic-segmentation/lib/thirdparty/bin/sum-tree-stats\n",
            "automatic-segmentation/lib/thirdparty/bin/subsample-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/post-to-weights\n",
            "automatic-segmentation/lib/thirdparty/bin/select-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/phonetisaurus-align\n",
            "automatic-segmentation/lib/thirdparty/bin/tree-info\n",
            "automatic-segmentation/lib/thirdparty/bin/weight-silence-post\n",
            "automatic-segmentation/lib/thirdparty/bin/nnet-train-parallel\n",
            "automatic-segmentation/lib/thirdparty/bin/scale-post\n",
            "automatic-segmentation/lib/thirdparty/bin/subset-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/transform-feats\n",
            "automatic-segmentation/lib/thirdparty/bin/vector-sum\n",
            "automatic-segmentation/lib/thirdparty/bin/phonetisaurus-g2pfst\n",
            "automatic-segmentation/lib/thirdparty/bin/sum-lda-accs\n",
            "automatic-segmentation/models/\n",
            "automatic-segmentation/models/pt.zip\n",
            "automatic-segmentation/models/hmm/\n",
            "automatic-segmentation/models/hmm/hmmdefs\n",
            "automatic-segmentation/models/hmm/macros\n",
            "automatic-segmentation/Dictionaries/\n",
            "automatic-segmentation/Dictionaries/Homographs.json\n",
            "automatic-segmentation/Dictionaries/Exceptions.txt\n",
            "automatic-segmentation/Dictionaries/Homographs.txt\n",
            "automatic-segmentation/Dictionaries/Exceptions.json\n",
            "automatic-segmentation/Dictionaries/Verbs.json\n",
            "automatic-segmentation/static/\n",
            "automatic-segmentation/static/table.png\n",
            "automatic-segmentation/static/styles/\n",
            "automatic-segmentation/static/styles/style.css\n",
            "automatic-segmentation/hash_map.py\n",
            "automatic-segmentation/Word.py\n",
            "automatic-segmentation/Conversor.py\n",
            "automatic-segmentation/TextConverter.py\n"
          ]
        }
      ],
      "source": [
        "!tar xzvf automatic-segmentation.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAYHWdVntCJk",
        "outputId": "eb8f8ded-274a-4205-8151-64c2ad39109e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-G2k2XJaO_Jvd4mQ37avufXihS0gKRLI\n",
            "To: /content/SP_D2_255_segmentado.tar.gz\n",
            "100% 680M/680M [00:07<00:00, 87.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1-G2k2XJaO_Jvd4mQ37avufXihS0gKRLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBULYuBItecJ",
        "outputId": "b6172919-ddce-4aae-bf84-3125c549c7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SP_D2_255_segmentado/\n",
            "SP_D2_255_segmentado/SP_D2_255.TextGrid\n",
            "SP_D2_255_segmentado/SP_D2_255_1/\n",
            "SP_D2_255_segmentado/SP_D2_255_1/SP_D2_255_clipped_1.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_1/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_1/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_1/locutores_palavras_align.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_1/locutores_palavras.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_1/SP_D2_255_clipped_1.lab\n",
            "SP_D2_255_segmentado/SP_D2_255_1/SP_D2_255_clipped_1.mfc\n",
            "SP_D2_255_segmentado/SP_D2_255_3/\n",
            "SP_D2_255_segmentado/SP_D2_255_3/SP_D2_255_clipped_3.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_3/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_3/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_8/\n",
            "SP_D2_255_segmentado/SP_D2_255_8/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_8/SP_D2_255_clipped_8.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_8/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_7/\n",
            "SP_D2_255_segmentado/SP_D2_255_7/SP_D2_255_clipped_7.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_7/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_7/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_4/\n",
            "SP_D2_255_segmentado/SP_D2_255_4/SP_D2_255_clipped_4.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_4/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_4/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_2/\n",
            "SP_D2_255_segmentado/SP_D2_255_2/SP_D2_255_clipped_2.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_2/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_2/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_6/\n",
            "SP_D2_255_segmentado/SP_D2_255_6/SP_D2_255_clipped_6.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_6/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_6/new_wavs/\n",
            "SP_D2_255_segmentado/SP_D2_255_5/\n",
            "SP_D2_255_segmentado/SP_D2_255_5/SP_D2_255_clipped_5.wav\n",
            "SP_D2_255_segmentado/SP_D2_255_5/locutores.txt\n",
            "SP_D2_255_segmentado/SP_D2_255_5/new_wavs/\n"
          ]
        }
      ],
      "source": [
        "!tar xzvf SP_D2_255_segmentado.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR2RJeWIsspt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E_ugdc_QBvaF"
      },
      "outputs": [],
      "source": [
        "rel_path = \"automatic-segmentation/\"\n",
        "rel_path_inq = \"SP_D2_255_segmentado/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ3epQeIqxQT",
        "outputId": "264c0847-c7d1-4cdd-be3b-d870bd04c37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sly\n",
            "  Downloading sly-0.5-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: sly\n",
            "Successfully installed sly-0.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py-espeak-ng\n",
            "  Downloading py_espeak_ng-0.1.8-py2.py3-none-any.whl (6.3 kB)\n",
            "Installing collected packages: py-espeak-ng\n",
            "Successfully installed py-espeak-ng-0.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting phonemizer\n",
            "  Downloading phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from phonemizer) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from phonemizer) (4.1.1)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.7/dist-packages (from phonemizer) (22.1.0)\n",
            "Collecting segments\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting dlinfo\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segments->phonemizer) (2022.6.2)\n",
            "Collecting csvw>=1.5.6\n",
            "  Downloading csvw-3.1.3-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting clldutils>=1.7.3\n",
            "  Downloading clldutils-3.12.0-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 48.0 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.8.10)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.8.2)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.10.3)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 667 kB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.3.3)\n",
            "Collecting language-tags\n",
            "  Downloading language_tags-1.1.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.23.0)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer) (3.0.1)\n",
            "Collecting rfc3986<2\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->csvw>=1.5.6->segments->phonemizer) (2022.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from isodate->csvw>=1.5.6->segments->phonemizer) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (4.13.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->csvw>=1.5.6->segments->phonemizer) (3.10.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (57.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2022.9.24)\n",
            "Installing collected packages: isodate, rfc3986, rdflib, language-tags, colorama, csvw, colorlog, clldutils, segments, dlinfo, phonemizer\n",
            "Successfully installed clldutils-3.12.0 colorama-0.4.6 colorlog-6.7.0 csvw-3.1.3 dlinfo-1.2.1 isodate-0.6.1 language-tags-1.1.0 phonemizer-3.2.1 rdflib-6.2.0 rfc3986-1.5.0 segments-2.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 9.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=7c036fbdd0a4f7357f11c789d533386a3473dd76becbca137e695571cdd951f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.56.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (4.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tgt\n",
            "  Downloading tgt-1.4.4.tar.gz (21 kB)\n",
            "Building wheels for collected packages: tgt\n",
            "  Building wheel for tgt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tgt: filename=tgt-1.4.4-py3-none-any.whl size=28928 sha256=d308ff6e3d060c32aefdef3f7ca558d5efb93428072473acf1a1e0f6bc8e68b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/26/00/05f728381a2620ac79029acb7eb117631a8d1046d0c603ab5e\n",
            "Successfully built tgt\n",
            "Installing collected packages: tgt\n",
            "Successfully installed tgt-1.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install sly\n",
        "!pip install py-espeak-ng\n",
        "!pip install phonemizer\n",
        "!pip install pydub\n",
        "!pip install numpy\n",
        "!pip install chardet\n",
        "!pip install pyspellchecker\n",
        "!pip install num2words\n",
        "!pip install librosa\n",
        "!pip install tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHqqpZHRys9C",
        "outputId": "f8a21a52-13fe-47cf-be74-f4739eb16cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0 python-espeak\n",
            "0 upgraded, 5 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 1,166 kB of archives.\n",
            "After this operation, 2,859 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-espeak amd64 0.5-1build5 [8,932 B]\n",
            "Fetched 1,166 kB in 1s (1,420 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package python-espeak.\n",
            "Preparing to unpack .../python-espeak_0.5-1build5_amd64.deb ...\n",
            "Unpacking python-espeak (0.5-1build5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up python-espeak (0.5-1build5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [98.9 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [992 kB]\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,472 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,040 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,267 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,226 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,210 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,130 kB]\n",
            "Fetched 17.6 MB in 5s (3,341 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  espeak\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 61.6 kB of archives.\n",
            "After this operation, 209 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 61.6 kB in 0s (196 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package espeak.\n",
            "(Reading database ... 124249 files and directories currently installed.)\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install python-espeak\n",
        "!sudo apt-get update && sudo apt-get install espeak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7pCn6t7xsJlx"
      },
      "outputs": [],
      "source": [
        "!cp automatic-segmentation/Conversor.py .\n",
        "!cp automatic-segmentation/TextConverter.py .\n",
        "!cp automatic-segmentation/Word.py .\n",
        "!cp automatic-segmentation/Parser.py .\n",
        "!cp automatic-segmentation/GramaticalClass.py .\n",
        "!cp automatic-segmentation/hash_map.py .\n",
        "!cp automatic-segmentation/phoneme_rules.py .\n",
        "!cp automatic-segmentation/Dictionaries/Homographs.json ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIZ2aI7zmtzF",
        "outputId": "2e407035-5728-4b05-c52f-9f24fab428c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: 15 shift/reduce conflicts\n"
          ]
        }
      ],
      "source": [
        "from spellchecker import SpellChecker\n",
        "from phonemizer.backend import EspeakBackend\n",
        "from Conversor import *\n",
        "from TextConverter import *\n",
        "import re\n",
        "from num2words import num2words\n",
        "from os import path, listdir\n",
        "import pydub\n",
        "import glob\n",
        "from os.path import isfile, join\n",
        "from collections import OrderedDict\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from scipy.io.wavfile import write\n",
        "import chardet\n",
        "import tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bQPiTqZZqeKH"
      },
      "outputs": [],
      "source": [
        "raw_vocab ='ABCDEFGHIJKLMNOPQRSTUVWXYZÇÃÀÁÂÊÉÍÓÔÕÚÛabcdefghijklmnopqrstuvwxyzçãàáâêéíóôõúû()\\-\\'\\n\\?\\./,\\'\\\": '\n",
        "clean_vocab ='ABCDEFGHIJKLMNOPQRSTUVWXYZÇÃÀÁÂÊÉÍÓÔÕÚÛabcdefghijklmnopqrstuvwxyzçãàáâêéíóôõúû\\-\\'\\n\\? '\n",
        "\n",
        "#############################################################\n",
        "# Linked segments lists\n",
        "#############################################################\n",
        "class AudioSegment:\n",
        "  def __init__(self, start, end):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.next = None\n",
        "    self.gap = 0 # gap between segments (current and next)\n",
        "\n",
        "  def set_next(self, next):\n",
        "    self.next = next\n",
        "    self.gap = next.start - self.end\n",
        "\n",
        "  def set_filename_and_id(self, filename, id):\n",
        "    self.filename = filename\n",
        "    self.id = id\n",
        "\n",
        "  def merge_from(self, next): \n",
        "    # merge two segments (current and next)\n",
        "    self.next = next.next\n",
        "    self.gap = next.gap\n",
        "    self.end = next.end\n",
        "\n",
        "  def duration(self, sample_rate):\n",
        "    return (self.end - self.start - 1) / sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tM0JTV0AqeuY"
      },
      "outputs": [],
      "source": [
        "class AutomaticSegmentation:\n",
        "    def __init__(self, path, audio_file, locs_file):\n",
        "        self.path = path\n",
        "        self.audio_file = audio_file\n",
        "        self.locs_file = locs_file\n",
        "        self.text_align = \"\"\n",
        "        self.silences_file = \"\"\n",
        "        self.alignment_tg = \"\"\n",
        "\n",
        "    # palavras_por_locutor\n",
        "    def clean_text(self, new_text):\n",
        "        if new_text[0] == ' ':\n",
        "            new_text = new_text[1:]\n",
        "\n",
        "        new_text = re.sub(\"ininteligível\", \"\", new_text)\n",
        "        new_text = re.sub(\"inint\", \"\", new_text)\n",
        "        new_text = re.sub(\"inint\\.\", \"\", new_text)\n",
        "\n",
        "        # Remove texto entre parênteses duplos\n",
        "        new_text = re.sub(\"\\(\\([^)]*\\)\\)\", \"\", new_text)\n",
        "\n",
        "        # Remove texto entre parênteses duplos e \"...\" (caso o transcritor tenha esquecido de fechar os parênteses)\n",
        "        new_text = re.sub(\"\\(\\([^(\\.\\.\\.)]*\\.\\.\\.\", \"\", new_text)\n",
        "\n",
        "        # Troca :: por espaço\n",
        "        new_text = re.sub(\"::\", \" \", new_text)\n",
        "\n",
        "        # Troca ` por '\n",
        "        new_text = new_text.replace(\"`\",\"'\")\n",
        "\n",
        "        # se não há texto, só pontuação, retornamos a string vazia \"\"\n",
        "        if not re.search('[A-Za-z0-9áàâãéèêíóôõúçÁÀÂÃÉÈÍÓÔÕÚÇ]', new_text):\n",
        "            return \"\"\n",
        "\n",
        "        # Formata conforme o vocabulário limpo\n",
        "        new_text = re.sub(\"[^{}]\".format(clean_vocab), \"\", new_text)\n",
        "\n",
        "        # Remove múltiplos espaços\n",
        "        new_text = re.sub(\"[ ]+\", \" \", new_text)\n",
        "\n",
        "        new_text = re.sub(\"(?<![A-Z])\\.\", \"\", new_text)\n",
        "        new_text = re.sub(\"\\n[ ]+\", \"\\n\", new_text)\n",
        "        new_text = re.sub(\"\\n{3, 6}\", \"\\n\\n\", new_text)\n",
        "        new_text = re.sub(\"[ ]+\", \" \", new_text)\n",
        "\n",
        "        # Substitui ehhhhhh por eh e afins    \n",
        "        new_text = re.sub(\"h+\", \"h\", new_text)\n",
        "\n",
        "        new_text = re.sub(' +', ' ', new_text)\n",
        "        new_text = new_text.replace(\"\\n \", \"\\n\")\n",
        "        \n",
        "        if len(new_text.split(\"\\n\")) > 0:\n",
        "            new_text = os.linesep.join([s for s in new_text.splitlines() if s])\n",
        "        return new_text\n",
        "\n",
        "    def filled_pause_normalization(self, word):\n",
        "        # éh, eh\n",
        "        filled_pause_eh = [\"éh\",\"ehm\",\"ehn\",\"he\",\"éhm\",\"éhn\",\"hé\"]\n",
        "        if word in filled_pause_eh:\n",
        "            word = \"eh\"\n",
        "\n",
        "        # uh, hum, hm, uhm\n",
        "        filled_pause_uh = [\"hum\",\"hm\",\"uhm\",\"hu\",\"uhn\"]\n",
        "        if word in filled_pause_uh:\n",
        "            word = \"uh\"\n",
        "\n",
        "        # uhum, aham\n",
        "        filled_pause_aham = [\"uhum\",\"uhun\",\"unhun\",\"unhum\",\"umhun\",\"umhum\",\n",
        "                             \"hunhun\",\"humhum\",\"hanhan\",\"ahan\",\"uhuhum\"]\n",
        "        if word in filled_pause_aham:\n",
        "            word = \"aham\"\n",
        "\n",
        "        # ah, hã, ãh, ã\n",
        "        filled_pause_ah = [\"hã\",\"ãh\",\"ã\",\"ah\",\"ahn\",\"han\",\"ham\"]\n",
        "        if word in filled_pause_ah:\n",
        "            word = \"ah\"\n",
        "        return word\n",
        "\n",
        "    # Se a palavra não é reconhecida pela lista de palavras da biblioteca, retorna a palavra corrigida.\n",
        "    #  Se não, retorna a mesma palavra passada por parâmetro\n",
        "    def spellcheck(self, word):\n",
        "        spell = SpellChecker(language='pt')\n",
        "        #print(\"spellchecking\", word)\n",
        "\n",
        "        fp_word = self.filled_pause_normalization(word)\n",
        "        if fp_word != word:\n",
        "            return fp_word\n",
        "\n",
        "        #if spell.unknown([word]):\n",
        "        #    if '-' or '?' in word:\n",
        "        #        return word\n",
        "        #    print(\"palavra \\\"\", word, \"\\\" não reconhecida convertida para\", spell.correction(word))\n",
        "        #    return spell.correction(word)\n",
        "        #else:\n",
        "        #    return word\n",
        "        return word\n",
        "\n",
        "    # Gera dois novos arquivos com as falas e os turnos\n",
        "    def generate_words_file(self, locs_file):\n",
        "        with open(locs_file, 'r') as lf:\n",
        "            linhas = lf.readlines()\n",
        "\n",
        "        is_new = True\n",
        "        locs_list = []\n",
        "        for i, l in enumerate(linhas):\n",
        "            if l == '\\n':\n",
        "                break\n",
        "            loc = l.split(';')[0]\n",
        "            if loc not in locs_list:\n",
        "                for ll in locs_list:\n",
        "                    # se o locutor atual é igual a um anterior com a adição ou remoção de um caractere \"-\" ou \".\", é o mesmo locutor, e não adicionamos novamente\n",
        "                    if loc == ll+\".\" or loc == ll+\"-\" or (ll[-1] == \".\" and ll[:len(loc)] == loc) or (ll[-1] == \"-\" and ll[:len(loc)] == loc):\n",
        "                        is_new = False\n",
        "                        print(\"mesmo locutor com nomes diferentes\", loc, ll)\n",
        "                        print(\"trocando\", l)\n",
        "                        linhas[i] = ll + \";\" + l.split(';')[1]\n",
        "                        print(\"por\", l)\n",
        "                        break\n",
        "                if is_new:\n",
        "                    print(\"loc appended\", loc)\n",
        "                    locs_list.append(loc)\n",
        "                is_new = True\n",
        "                        \n",
        "\n",
        "        with open(locs_file.replace(\".txt\", \"_palavras_align.txt\"), 'w') as nlf2:\n",
        "            with open(locs_file.replace(\".txt\", \"_palavras.txt\"), 'w') as nlf:\n",
        "                for l in linhas:\n",
        "                    loc = l.split(\";\")[0]\n",
        "                    l = l.split(\";\")[1]\n",
        "                    l = l.lower()\n",
        "                    # remove indicação do locutor no início da frase\n",
        "                    for iloc in locs_list:\n",
        "                        if iloc in l:\n",
        "                            l = l.replace(iloc, \"\")\n",
        "                    l = self.clean_text(l)\n",
        "                    for lp in l.split():\n",
        "                        #lp = self.spellcheck(lp)\n",
        "                        # se palavra é só um número continua sem escrever\n",
        "                        if lp.isnumeric():\n",
        "                            continue\n",
        "                        nlf.write(loc+';'+lp+\"\\n\")\n",
        "                        nlf2.write(lp+\"\\n\")\n",
        "        self.text_align = locs_file.replace(\".txt\", \"_palavras_align.txt\")\n",
        "\n",
        "    # alinhador_fonetico\n",
        "    def clean_tg(self):\n",
        "\t    if('win' not in sys.platform.lower()):\n",
        "\t\t    subprocess.run(['rm','-f', '*.TextGrid'])\n",
        "\t\t    subprocess.run(['rm','-f','-r','output'])\n",
        "\t    else:\n",
        "\t\t    os.system(\" \".join(['del', '*.TextGrid', '>NUL 2>&1']))\n",
        "\t\t    #subprocess.run(['del',\\\n",
        "\t\t    #\t\t'*.wav',\\\n",
        "\t\t    #\t\t'a',\\\n",
        "\t\t    #\t\t'b',\\\n",
        "\t\t    #\t\t'*.mlf',\\\n",
        "\t\t    #\t\t'dict',\\\n",
        "\t\t    #\t\t'*.out',\\\n",
        "\t\t    #\t\t'*.lab',\\\n",
        "\t\t    #\t\t'*.scp',\\\n",
        "\t\t    #\t\t'*.matl',\\\n",
        "\t\t    #\t\t'*.mfc',\\\n",
        "\t\t    #\t\t'hmmdefs',\\\n",
        "\t\t    #\t\t'*.TextGrid'],\\\n",
        "\t\t    #\t\tstdout=subprocess.DEVNULL,\\\n",
        "\t\t    #\t\tstderr=subprocess.DEVNULL)\n",
        "\n",
        "    def clean(self, rel_path):\n",
        "        TMP = os.path.join(rel_path,'tmp/')\n",
        "        if('win' not in sys.platform.lower()):\n",
        "            subprocess.run(['rm',\\\n",
        "                    '-f',\\\n",
        "                    os.path.join(TMP,'*.wav'),\\\n",
        "                    os.path.join(TMP,'a'),\\\n",
        "                    os.path.join(TMP,'b'),\\\n",
        "                    os.path.join(TMP,'dict'),\\\n",
        "                    os.path.join(TMP,'*.mlf'),\\\n",
        "                    os.path.join(TMP,'*.out'),\\\n",
        "                    os.path.join(TMP,'*.lab'),\\\n",
        "                    os.path.join(TMP,'*.scp'),\\\n",
        "                    os.path.join(TMP,'*.matl'),\\\n",
        "                    os.path.join(TMP,'*.mfc'),\\\n",
        "                    os.path.join(TMP,'hmmdefs')])\n",
        "            subprocess.run(['rm','-f','-r','output'])\n",
        "        else:\n",
        "            os.system(\" \".join(['del',\\\n",
        "                    os.path.join(TMP,'*.wav'),\\\n",
        "                    os.path.join(TMP,'a'),\\\n",
        "                    os.path.join(TMP,'b'),\\\n",
        "                    os.path.join(TMP,'*.mlf'),\\\n",
        "                    os.path.join(TMP,'dict'),\\\n",
        "                    os.path.join(TMP,'*.out'),\\\n",
        "                    os.path.join(TMP,'*.lab'),\\\n",
        "                    os.path.join(TMP,'*.scp'),\\\n",
        "                    os.path.join(TMP,'*.matl'),\\\n",
        "                    os.path.join(TMP,'*.mfc'),\\\n",
        "                    os.path.join(TMP,'hmmdefs'),\\\n",
        "                    '>NUL 2>&1']))\n",
        "\n",
        "    def align(self, audio_file, text_align, rel_path):\n",
        "        TC = TextConverter(rel_path)\n",
        "        self.clean_tg()\n",
        "\n",
        "        with open(text_align, 'r') as tf:\n",
        "            text = tf.read()\n",
        "\n",
        "        hmmdefs = False\n",
        "        req_in = 'graf'\n",
        "        req_out = 'fonema'\n",
        "        aligner = 'HTK'\n",
        "\n",
        "        # Cria todos os arquivos necessarios para o alinhamento\t\n",
        "        text1 = TC.perform_conversion(text,\n",
        "                    audio_file,\n",
        "                    req_in,\n",
        "                    req_out,\n",
        "                    rel_path,\n",
        "                    aligner)\n",
        "\n",
        "        TC.align(text, audio_file, req_in, req_out, rel_path, rel_path+\"tmp/\", aligner=aligner, hmmdefs=hmmdefs)\n",
        "        print(\"alinhamento feito\")\n",
        "\n",
        "        # Formata o arquivo de saida para o formato desejado\n",
        "        TC.format_output(text1, audio_file, req_out, rel_path, aligner)\n",
        "        print(\"saidas geradas\")\n",
        "\n",
        "        self.alignment_tg = self.audio_file.replace(\".wav\", \".TextGrid\")\n",
        "\n",
        "        self.clean(rel_path)\n",
        "\n",
        "    #############################################################\n",
        "    # Segment audio file and return a segment linked list\n",
        "    #############################################################\n",
        "    def segment_wav(self, wav, threshold_db):\n",
        "      # Find gaps at a fine resolution:\n",
        "      parts = librosa.effects.split(wav, top_db=threshold_db, frame_length=1024, hop_length=256)\n",
        "\n",
        "      # Build up a linked list of segments:\n",
        "      head = None\n",
        "      for start, end in parts:\n",
        "        segment = AudioSegment(start, end)\n",
        "        if head is None:\n",
        "          head = segment\n",
        "        else:\n",
        "          prev.set_next(segment)\n",
        "        prev = segment\n",
        "      return head\n",
        "\n",
        "    #############################################################\n",
        "    # Given an audio file, creates the best possible segment list \n",
        "    #############################################################\n",
        "    def find_segments(self, filename, wav, sample_rate, min_duration, max_duration, max_gap_duration, threshold_db, wav_dest_dir):\n",
        "      # Segment audio file\n",
        "      segments = self.segment_wav(wav, threshold_db)\n",
        "\n",
        "      # Convert to list\n",
        "      result = []\n",
        "      s = segments\n",
        "      while s is not None:\n",
        "        result.append(s)\n",
        "        # Create a errors file\n",
        "        if (s.duration(sample_rate) < min_duration and\n",
        "            s.duration(sample_rate) > max_duration):\n",
        "            with open(wav_dest_dir+\"/erros.txt\", \"a\") as f:\n",
        "                f.write(filename+\"\\n\")\n",
        "        s = s.next\n",
        "\n",
        "      with open(wav_dest_dir+'/'+\"silences.txt\", \"w\") as sf:\n",
        "        for r in result:\n",
        "          sf.write(str(r.start/sample_rate) + ' ' + str(r.end/sample_rate) + \"\\n\")\n",
        "\n",
        "      return result\n",
        "\n",
        "    #############################################################\n",
        "    # Given an folder, creates a wav file alphabetical order dict  \n",
        "    #############################################################\n",
        "    def load_filenames(self, base_dir, orig):\n",
        "      mappings = OrderedDict()\n",
        "      for filepath in glob.glob(join(base_dir, orig + \"/*.wav\")):\n",
        "        filename = filepath.split('/')[-1].split('.')[0]\n",
        "        mappings[filename] = filepath\n",
        "      return mappings\n",
        "\n",
        "    #############################################################\n",
        "    # Build best segments of wav files  \n",
        "    #############################################################\n",
        "    def find_silences(self, base_dir, orig, dest, sampling_rate, min_duration, max_duration, max_gap_duration, threshold, output_filename, output_filename_id):\n",
        "      # Creates destination folder\n",
        "      wav_dest_dir = os.path.join(base_dir, dest)\n",
        "      os.makedirs(wav_dest_dir, exist_ok=True)\n",
        "      # Initializes variables\n",
        "      max_duration, mean_duration = 0, 0\n",
        "      all_segments = []\n",
        "      total_duration = 0\n",
        "      filenames = self.load_filenames(base_dir, orig)\n",
        "      for i, (file_id, filename) in enumerate(filenames.items()):\n",
        "        print('Loading %s: %s (%d of %d)' % (file_id, filename, i+1, len(filenames)))\n",
        "        wav, sample_rate = librosa.load(filename, sr=sampling_rate)\n",
        "        print(' -> Loaded %.1f min of audio. Splitting...' % (len(wav) / sample_rate / 60))\n",
        "\n",
        "        # Find best segments\n",
        "        segments = self.find_segments(filename, wav, sample_rate, min_duration, max_duration,\n",
        "          max_gap_duration, threshold, wav_dest_dir)\n",
        "        duration = sum((s.duration(sample_rate) for s in segments))\n",
        "        total_duration += duration\n",
        "\n",
        "        # Create records for the segments\n",
        "        output_filename = output_filename  if output_filename else file_id\n",
        "        j = int(output_filename_id)\n",
        "        for s in segments:\n",
        "          all_segments.append(s)\n",
        "          s.set_filename_and_id(filename, '%s-%04d' % (output_filename, j))\n",
        "          j = j + 1\n",
        "\n",
        "        print(' -> Segmented into %d parts (%.1f min, %.2f sec avg)' % (\n",
        "          len(segments), duration / 60, duration / len(segments)))\n",
        "\n",
        "        # Write segments to disk:\n",
        "        for s in segments:\n",
        "          #segment_wav = (wav[s.start:s.end] * 32767).astype(np.int16)\n",
        "          segment_wav = (wav[s.start:s.end] * 32767).astype(np.int16)\n",
        "          out_path = os.path.join(wav_dest_dir, '%s.wav' % s.id)\n",
        "          #librosa.output.write_wav(out_path, segment_wav, sample_rate)\n",
        "          #write(out_path, sample_rate, segment_wav)\n",
        "\n",
        "          duration += len(segment_wav) / sample_rate\n",
        "          duration_segment = len(segment_wav) / sample_rate\n",
        "          if duration_segment > max_duration:\n",
        "            max_duration = duration_segment\n",
        "\n",
        "          mean_duration = mean_duration + duration_segment\n",
        "        print(' -> Wrote %d segment wav files' % len(segments))\n",
        "        print(' -> Progress: %d segments, %.2f hours, %.2f sec avg' % (\n",
        "          len(all_segments), total_duration / 3600, total_duration / len(all_segments)))\n",
        "\n",
        "      print('Writing metadata for %d segments (%.2f hours)' % (len(all_segments), total_duration / 3600))\n",
        "      with open(os.path.join(base_dir, 'segments.csv'), 'w') as f:\n",
        "        for s in all_segments:\n",
        "          f.write('%s|%s|%d|%d\\n' % (s.id, s.filename, s.start, s.end))\n",
        "      print('Mean: %f' %( mean_duration ))\n",
        "      print('Max: %d' %(max_duration ))\n",
        "      self.silences_file = self.path+\"silences.txt\"\n",
        "\n",
        "    def predict_encoding(self, tg_path):\n",
        "        '''Predict a file's encoding using chardet'''\n",
        "        # Open the file as binary data\n",
        "        with open(tg_path, 'rb') as f:\n",
        "            # Join binary lines for specified number of lines\n",
        "            rawdata = b''.join(f.readlines())\n",
        "\n",
        "        return chardet.detect(rawdata)['encoding']\n",
        "\n",
        "    def calculate_average_phone_duration(self, window_phones):\n",
        "        s = 0\n",
        "        for wp in window_phones:\n",
        "            s += wp[1]\n",
        "        return s / len(window_phones)\n",
        "\n",
        "    def dsr_threshold_1(self, windows, boundaries_tier_1, delta1):\n",
        "        # primeiro encontramos o maior e menor speech rates no turno\n",
        "        max_sr_diff = 0\n",
        "        last_sr = 0\n",
        "        for w in windows:\n",
        "            #print(w)\n",
        "            if abs(w[1] - last_sr) > max_sr_diff:\n",
        "                max_sr_diff = abs(w[1] - last_sr)\n",
        "            last_sr = w[1]\n",
        "\n",
        "        # se a diferença entre os speech rates das janelas consecutivas é > delta1 da maior diferença entre speech rates,\n",
        "        #  identificamos como DSR.\n",
        "        last_sr = 0\n",
        "        dsrs_1 = []\n",
        "        dsr_windows_1 = []\n",
        "        # tempo da última fronteira\n",
        "        last_boundary = 0\n",
        "        for w in windows:\n",
        "            print(abs(w[1] - last_sr), \", threshold is\", delta1, \"*\", max_sr_diff, \"=\", delta1 * max_sr_diff)\n",
        "            if abs(w[1] - last_sr) > delta1 * max_sr_diff:\n",
        "                print(\"DSR!\", w)\n",
        "                print(\"adding boundary to tier 1\")\n",
        "                boundary = tgt.core.Interval(start_time=last_boundary, end_time=w[0][0][2])\n",
        "                last_boundary = w[0][0][2]\n",
        "                try:\n",
        "                    boundaries_tier_1.add_interval(boundary)\n",
        "                except:\n",
        "                    print(\"overlap!\")\n",
        "                dsrs_1.append(w[0][0][2])\n",
        "                dsr_windows_1.append(w)\n",
        "            last_sr = w[1]\n",
        "\n",
        "        return dsrs_1, dsr_windows_1\n",
        "\n",
        "    def dsr_threshold_2(self, dsr_windows_1, boundaries_tier_2, delta2, interval_size):\n",
        "        max_sr = 0\n",
        "        min_sr = 9999\n",
        "        for dsr in dsr_windows_1:\n",
        "            #print(w)\n",
        "            if dsr[1] > max_sr:\n",
        "                max_sr = dsr[1]\n",
        "            if dsr[1] < min_sr:\n",
        "                min_sr = dsr[1]\n",
        "\n",
        "        # se a diferença entre os speech rates das janelas consecutivas é > delta2 da maior diferença entre speech rates,\n",
        "        #  identificamos como DSR.\n",
        "        last_dsr = 0\n",
        "        filtered = []\n",
        "        for dsr in dsr_windows_1:\n",
        "            if dsr[0][0][2] - last_dsr > interval_size:\n",
        "                filtered.append(dsr)\n",
        "            last_dsr = dsr[0][0][3]\n",
        "\n",
        "        dsrs_2 = []\n",
        "        last_sr = 0\n",
        "        last_boundary = 0\n",
        "        for w in filtered:\n",
        "            if abs(w[1] - last_sr) > delta2 * (max_sr - min_sr):\n",
        "                print(\"DSR!\", w)\n",
        "                print(\"adding boundary to tier 2\")\n",
        "                boundary = tgt.core.Interval(start_time=last_boundary, end_time=w[0][0][2])\n",
        "                last_boundary = w[0][0][2]\n",
        "                try:\n",
        "                    boundaries_tier_2.add_interval(boundary)\n",
        "                except:\n",
        "                    print(\"overlap!\")\n",
        "                dsrs_2.append(w[0][0][2])\n",
        "            last_sr = w[1]\n",
        "        return dsrs_2\n",
        "\n",
        "    def print_silences(self, sil_file, boundaries_tier_3, silence_threshold):\n",
        "        with open(sil_file, \"r\") as sf:\n",
        "            sils = sf.readlines()\n",
        "        \n",
        "        silences = []\n",
        "        last_boundary = 0\n",
        "        for s in sils:\n",
        "            #print(\"intervalo de fala\", s)\n",
        "            interval = s.split()\n",
        "            #print(\"trecho de silêncio entre\", last_boundary, \"e\", float(interval[0]), \"com duração\", float(interval[0]) - last_boundary)\n",
        "            if float(interval[0]) - last_boundary > silence_threshold:\n",
        "                print(\"DSR!\", interval[0])\n",
        "                #print(\"adding boundary to tier 3\")\n",
        "                boundary = tgt.core.Interval(start_time=last_boundary, end_time=float(interval[0]))\n",
        "                last_boundary = float(interval[1])\n",
        "                try:\n",
        "                    boundaries_tier_3.add_interval(boundary)\n",
        "                except:\n",
        "                    print(\"overlap!\")\n",
        "                silences.append(float(interval[0]))\n",
        "        return silences\n",
        "\n",
        "    def fill_boundaries_tier(self, timestamps, boundaries_tier):\n",
        "        timestamps.sort()\n",
        "\n",
        "        #print(\"timestamps:\", timestamps)\n",
        "\n",
        "        last_ts = timestamps[0]\n",
        "        for ts in timestamps[1:]:\n",
        "            #print(\"boundary:\", last_ts, ts)\n",
        "            boundary = tgt.core.Interval(start_time=last_ts, end_time=ts)\n",
        "            boundaries_tier.add_interval(boundary)\n",
        "            last_ts = ts\n",
        "\n",
        "    def find_boundaries(self, locs_file, tg_file, sil_file, window_size, delta1, delta2, interval_size, silence_threshold, min_words_h2):\n",
        "        tg = tgt.io.read_textgrid(tg_file, self.predict_encoding(tg_file), include_empty_intervals=False)\n",
        "        tier_names = []\n",
        "        boundaries_tier_1 = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_heuristica_1\", objects=None)\n",
        "        boundaries_tier_2 = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_heuristica_2\", objects=None)\n",
        "        boundaries_tier_3 = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_heuristica_3\", objects=None)\n",
        "        boundaries_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"fronteiras_metodo\", objects=None)\n",
        "        names = tg.get_tier_names()\n",
        "        fim = False\n",
        "\n",
        "        # lemos as palavras no arquivo com locutores\n",
        "        with open(locs_file, 'r') as lf:\n",
        "            locs_and_words = lf.readlines()\n",
        "\n",
        "            # lemos as palavras na coversão g2p\n",
        "            Conv = Conversor()\n",
        "            sentences = \"\"\n",
        "            for lw in locs_and_words:\n",
        "                w = lw.split(';')[1]\n",
        "                sentences += ' ' + w\n",
        "            #print(\"sentences:\", sentences)\n",
        "            g2p_words = Conv.convert_sentence(sentences, rel_path)\n",
        "            sentences = sentences.split()\n",
        "            #print(\"sentences converted:\", g2p_words)\n",
        "            g2p_words = g2p_words.split()\n",
        "            for pwi, pw in enumerate(g2p_words):\n",
        "                # substituimos fonemas 'w' por 'v' (e 'y' por 'i') pois o alinhador joga fora (é uma solução tosca, mas o que dá pra fazer sem alterar o alinhador)\n",
        "                g2p_words[pwi] = g2p_words[pwi].replace(\"w\", \"v\")\n",
        "                g2p_words[pwi] = g2p_words[pwi].replace(\"y\", \"i\")\n",
        "                \n",
        "            print(\"g2p_words -> lista:\", g2p_words)\n",
        "            \n",
        "        for name in names:\n",
        "            tier = tg.get_tier_by_name(name)\n",
        "            \n",
        "            # índice para iterar pelas palavras convertidas via g2p\n",
        "            i = 0\n",
        "            curr_turn_start = 0.0\n",
        "            curr_turn = \"\"\n",
        "            window_phones = []\n",
        "            tier_names = []\n",
        "            windows = []\n",
        "            all_timestamps = []\n",
        "            constr = \"\"\n",
        "            first_phone = True\n",
        "            curr_word = g2p_words[0]\n",
        "            curr_word_grapheme = sentences[0]\n",
        "            curr_loc = locs_and_words[0].split(';')[0]\n",
        "            turn_index = 0\n",
        "            last_turn_start = 0\n",
        "\n",
        "            # estrutura que guarda, em ordem, cada palavra do texto, o texto do turno atual até dada palavra, o locutor desse turno,\n",
        "            #  o início do tempo da palavra, o início do tempo do turno e o final do tempo do turno até a palavra (final do tempo da palavra)\n",
        "            turn_until_word = []\n",
        "            for w in sentences:\n",
        "                turn_until_word.append([\"\", \"\", \"\", 0.0, 0.0, 0.0])\n",
        "            turn_until_word[0] = [curr_word_grapheme, curr_word_grapheme, curr_loc, 0.0, 0.0, 0.0]            \n",
        "\n",
        "            for enum, interval in enumerate(tier.intervals):\n",
        "                #print(\"fonema:\", interval.text)\n",
        "                if enum == 0 or enum == len(tier.intervals)-1:\n",
        "                    continue\n",
        "                if first_phone:\n",
        "                    curr_window = [interval.start_time, interval.start_time + window_size]\n",
        "                    first_phone = False\n",
        "                \n",
        "                constr += interval.text\n",
        "                if len(constr) < 20:\n",
        "                    print(\"curr_word:\", curr_word, \"; constr:\", constr)\n",
        "                \n",
        "                # adicionamos à lista de fonemas da janela o fonema atual e sua duração caso esteja dentro do tempo da janela\n",
        "                if interval.end_time < curr_window[1]:\n",
        "                    window_phones.append([interval.text, interval.end_time - interval.start_time, interval.start_time, interval.end_time, curr_loc])\n",
        "                \n",
        "                # se os fonemas encontrados desde a última janela formam a próxima palavra, pulamos para a próxima janela\n",
        "                if constr == curr_word:\n",
        "                    curr_turn += ' ' + curr_word_grapheme\n",
        "                    #print(\"window phones:\", window_phones)\n",
        "                    if window_phones:                \n",
        "                        av = self.calculate_average_phone_duration(window_phones)\n",
        "                        windows.append([window_phones, av])\n",
        "\n",
        "                    # atualiza o turno atual até a palavra e o tempo do final do turno até agora\n",
        "                    turn_until_word[i][1] = curr_turn\n",
        "                    turn_until_word[i][5] = interval.end_time\n",
        "                    \n",
        "                    # se a palavra atual foi concluída, pulamos para a próxima\n",
        "                    i += 1\n",
        "                    try:\n",
        "                        curr_word = g2p_words[i]\n",
        "                        curr_word_grapheme = sentences[i]\n",
        "                        #print(curr_word)\n",
        "\n",
        "                        # atualiza a palavra atual no turno e seu tempo de início \n",
        "                        turn_until_word[i][0] = curr_word_grapheme\n",
        "                        turn_until_word[i][3] = interval.end_time\n",
        "                        turn_until_word[i][4] = last_turn_start\n",
        "                        print(\"turn_until_word:\", turn_until_word[i])\n",
        "                    except:\n",
        "                        print(\"lista de g2p acabou\")\n",
        "                        fim = True\n",
        "                        #print(\"turno de\", curr_loc, windows)\n",
        "\n",
        "                        # primeira heurística\n",
        "                        dsrs_1, dsr_windows_1 = self.dsr_threshold_1(windows, boundaries_tier_1, delta1)\n",
        "                        # segunda heurística\n",
        "                        dsrs_2 = self.dsr_threshold_2(dsr_windows_1, boundaries_tier_2, delta2, interval_size)\n",
        "\n",
        "                        # junta todas as fronteiras identificadas pelas duas primeiras heuristicas aplicadas no turno em uma lista\n",
        "                        timestamps = list(set(dsrs_1 + dsrs_2))\n",
        "                        print(\"tamanho dsrs1:\", len(dsrs_1))\n",
        "                        print(\"tamanho dsrs2:\", len(dsrs_2))\n",
        "                        print(\"tamanho timestamps:\", len(timestamps), \"\\n\")\n",
        "                        all_timestamps += timestamps\n",
        "\n",
        "                        # limpa lista de janelas do turno\n",
        "                        windows = []\n",
        "                        break\n",
        "\n",
        "                    print(g2p_words[i], \":\", locs_and_words[i].split(';')[0])\n",
        "                    # se há troca de turno chamamos as heurísticas para as janelas do turno\n",
        "                    if locs_and_words[i].split(';')[0] != curr_loc:\n",
        "                        print(\"turno de\", curr_loc, windows)\n",
        "\n",
        "                        # primeira heurística\n",
        "                        dsrs_1, dsr_windows_1 = self.dsr_threshold_1(windows, boundaries_tier_1, delta1)\n",
        "                        # segunda heurística\n",
        "                        dsrs_2 = self.dsr_threshold_2(dsr_windows_1, boundaries_tier_2, delta2, interval_size)\n",
        "\n",
        "                        # junta todas as fronteiras identificadas pelas duas primeiras heuristicas aplicadas no turno em uma lista\n",
        "                        timestamps = list(set(dsrs_1 + dsrs_2))\n",
        "                        print(\"tamanho dsrs1:\", len(dsrs_1))\n",
        "                        print(\"tamanho dsrs2:\", len(dsrs_2))\n",
        "                        print(\"tamanho timestamps:\", len(timestamps), \"\\n\")\n",
        "                        all_timestamps += timestamps\n",
        "\n",
        "                        # limpa lista de janelas do turno\n",
        "                        windows = []\n",
        "\n",
        "                        if curr_loc in tier_names:\n",
        "                            loc_tb_tier = tg.get_tier_by_name(\"TB-\"+curr_loc)\n",
        "                            loc_ntb_tier = tg.get_tier_by_name(\"NTB-\"+curr_loc)\n",
        "                        else:\n",
        "                            # Creates TB and NTB tiers for the speaker\n",
        "                            loc_tb_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"TB-\"+curr_loc, objects=None)\n",
        "                            loc_ntb_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"NTB-\"+curr_loc, objects=None)\n",
        "\n",
        "                            # Adds the new tiers to the textgrid file\n",
        "                            tg.add_tier(loc_tb_tier)\n",
        "                            tg.add_tier(loc_ntb_tier)\n",
        "                            tier_names.append(curr_loc)                   \n",
        "\n",
        "                        # Resets the new tiers variables\n",
        "                        loc_tb_tier = None\n",
        "                        loc_ntb_tier = None\n",
        "                        curr_turn = \"\"\n",
        "\n",
        "                        # atualiza o tempo de início do próximo intervalo\n",
        "                        curr_turn_start = interval.start_time\n",
        "                        print(\"curr_turn_start\", curr_turn_start)\n",
        "                        # atualiza o começo do tempo do turno atual\n",
        "                        turn_until_word[i][4] = curr_turn_start\n",
        "                        last_turn_start = curr_turn_start\n",
        "\n",
        "                    # atualiza locutor para a proxima palavra\n",
        "                    curr_loc = locs_and_words[i].split(';')[0]\n",
        "\n",
        "                    # atualiza o locutor do turno atual, o tempo do início do turno e o texto do turno até a antiga palavra\n",
        "                    turn_until_word[i][2] = curr_loc\n",
        "\n",
        "                    # limpamos a string que guarda a palavra sendo construída pelos fonemas\n",
        "                    constr = \"\"        \n",
        "                    # janelas de 300 ms\n",
        "                    curr_window = [interval.end_time, interval.end_time + window_size]\n",
        "                    # limpamos a lista de fonemas para a próxima janela\n",
        "                    window_phones = []\n",
        "\n",
        "            # terceira heurística\n",
        "            silences = self.print_silences(sil_file, boundaries_tier_3, silence_threshold)\n",
        "\n",
        "            # junta todas as fronteiras identificadas das primeiras heuristicas com a terceira\n",
        "            all_timestamps = list(set(all_timestamps + silences))\n",
        "        \n",
        "            # preenche tier de boundaries juntando as 3 heurísticas\n",
        "            self.fill_boundaries_tier(all_timestamps, boundaries_tier)\n",
        "            \n",
        "        #tg.add_tier(boundaries_tier_1)\n",
        "        #tg.add_tier(boundaries_tier_2)\n",
        "        #tg.add_tier(boundaries_tier_3)\n",
        "\n",
        "        tg.add_tier(boundaries_tier)\n",
        "\n",
        "        last_c = 0\n",
        "        last_text = \"\"\n",
        "        last_loc = turn_until_word[0][2]\n",
        "        last_b = 0.0\n",
        "                \n",
        "        new_intervals = []\n",
        "        # aqui vamos inserir as informações das fronteiras identificadas pelo método nas tiers correspondentes de cada turno no textgrid\n",
        "        for b in boundaries_tier.intervals:\n",
        "            #print(\"boundary:\", b)\n",
        "            # itera pelas palavras\n",
        "            for c, t in enumerate(turn_until_word):\n",
        "                #print(\"inicio de t:\", t[3])\n",
        "                # se o início da fronteira ocorre após o início da palavra atual, pegue o turno até essa palavra\n",
        "                if b.start_time <= t[3]:\n",
        "                    # se trocou de turno zera o último texto\n",
        "                    if t[2] != last_loc:\n",
        "                        # quando troca de turno adicionamos o que resta do texto do turno anterior ao intervalo anterior\n",
        "                        nc = last_c\n",
        "                        while turn_until_word[nc][2] == last_loc:\n",
        "                            nc += 1\n",
        "                        # pega o texto restante do turno anterior após o último trecho de texto para adicionar retroativamente no último intervalo\n",
        "                        tail_text = turn_until_word[nc-1][1][len(last_text):]\n",
        "                        #print(\"turno completo:\", turn_until_word[nc-1][1])\n",
        "                        #print(\"tail text\", tail_text)\n",
        "                        last_i_updated = [tgt.core.Interval(start_time=last_i[0].start_time, end_time=last_i[0].end_time, text=last_i[0].text+' '+tail_text), last_loc]\n",
        "                        new_intervals[last_nic] = last_i_updated\n",
        "\n",
        "                        #print(\"updated:\", last_i_updated[0])\n",
        "\n",
        "                        # cria intervalo com turno até a fronteira\n",
        "                        i = [tgt.core.Interval(start_time=last_b, end_time=b.start_time, text=t[1]), t[2]]\n",
        "\n",
        "                        last_text = \"\"\n",
        "                    else:\n",
        "                        # cria intervalo com turno até a fronteira usando o texto do trecho atual menos o anterior\n",
        "                        i_text = t[1][len(last_text):]\n",
        "                        i = [tgt.core.Interval(start_time=last_b, end_time=b.start_time, text=i_text), t[2]]\n",
        "\n",
        "                    print(\"adicionando intervalo novo\", i)\n",
        "\n",
        "                    # adiciona intervalo nas duas camadas do turno\n",
        "                    new_intervals.append(i)\n",
        "    \n",
        "                    # salva o turno até a fronteira atual e o locutor            \n",
        "                    last_text = t[1]\n",
        "                    last_loc = t[2]\n",
        "                    last_b = b.start_time\n",
        "                    last_i = i\n",
        "                    last_c = c\n",
        "                    last_nic = len(new_intervals)-1\n",
        "\n",
        "                    # para de iterar pelas palavras para essa fronteira pois já foi encontrada\n",
        "                    break\n",
        "\n",
        "        first = True\n",
        "        # incluindo o final do texto no textgrid\n",
        "        if new_intervals[-1][0].end_time != tg.end_time:\n",
        "            nc = last_c+1\n",
        "            curr_i = [tgt.core.Interval(start_time=turn_until_word[last_c][4], end_time=turn_until_word[last_c][3], text=turn_until_word[last_c][1]), turn_until_word[last_c][2]]\n",
        "            while turn_until_word[nc][3] < tg.end_time:\n",
        "                if turn_until_word[nc][2] != last_loc:\n",
        "                    if first:\n",
        "                        curr_i[0].start_time = last_b\n",
        "                        first = False\n",
        "                    if curr_i[0].text != \"\" and curr_i[0].end_time != 0.0:\n",
        "                            new_intervals.append(curr_i)\n",
        "                #print(\"last loop\", turn_until_word[nc], \"last_loc=\", last_loc)\n",
        "                curr_i = [tgt.core.Interval(start_time=turn_until_word[nc][4], end_time=turn_until_word[nc][3], text=turn_until_word[nc][1]), turn_until_word[nc][2]]\n",
        "                last_loc = turn_until_word[nc][2]\n",
        "                nc += 1\n",
        "                if nc >= len(turn_until_word):\n",
        "                    try:\n",
        "                        #print(\"last i\", curr_i)\n",
        "                        if curr_i[0].text != \"\" and curr_i[0].end_time != 0.0:\n",
        "                            new_intervals.append(curr_i)\n",
        "                    except:\n",
        "                        print(\"overlap\")\n",
        "                    break\n",
        "\n",
        "        # adiciona intervalos nas duas camadas do turno adequado\n",
        "        for ni in new_intervals:\n",
        "            tb_turn_tier = tg.get_tier_by_name(\"TB-\"+ni[1])\n",
        "            ntb_turn_tier =  tg.get_tier_by_name(\"NTB-\"+ni[1])\n",
        "\n",
        "            tb_turn_tier.add_interval(ni[0])\n",
        "            ntb_turn_tier.add_interval(ni[0])\n",
        "\n",
        "        #deleta tier com alinhamento fonético e tier original colapsada com fronteiras do metodo\n",
        "        tg.delete_tier(\"labels\")\n",
        "        tg.delete_tier(\"fronteiras_metodo\")\n",
        "\n",
        "        # adiciona tier para comentários dos anotadores\n",
        "        comments_tier = tgt.core.IntervalTier(start_time=tg.start_time, end_time=tg.end_time, name=\"comentarios-anotacao\", objects=None)\n",
        "        tg.add_tier(comments_tier)\n",
        "\n",
        "        for name in tg.get_tier_names():\n",
        "            print(name)\n",
        "\n",
        "        tgt.io.write_to_file(tg, tg_file.replace(\".TextGrid\", \"_novo.TextGrid\"), format='long', encoding='utf-8')\n",
        "\n",
        "    def ser(self, annot_tg, method_tg, boundary_type):\n",
        "        if boundary_type not in [\"TB\", \"NTB\"]:\n",
        "            print(\"boundary_type inválido\")\n",
        "            return 0\n",
        "\n",
        "        Annot_tg = tgt.io.read_textgrid(annot_tg, self.predict_encoding(annot_tg), include_empty_intervals=True)\n",
        "        Method_tg = tgt.io.read_textgrid(method_tg, self.predict_encoding(method_tg), include_empty_intervals=True)\n",
        "        print(\"Method tg\", Method_tg)\n",
        "        \n",
        "        agreement_tier = tgt.core.IntervalTier(start_time=Annot_tg.start_time, end_time=Annot_tg.end_time, name=\"concordancia\", objects=None)\n",
        "\n",
        "        names_annot = Annot_tg.get_tier_names()\n",
        "        names_method = Method_tg.get_tier_names()\n",
        "        \n",
        "        method_boundaries = []\n",
        "        for name in names_method:\n",
        "            tier = Method_tg.get_tier_by_name(name)\n",
        "            if boundary_type in name:\n",
        "                print(\"tier\", name, \"adicionada:\", tier)\n",
        "                for interval in tier.intervals:\n",
        "                    print(\"intervalo do metodo\", interval)\n",
        "                    method_boundaries.append([interval, '0'])\n",
        "                Annot_tg.add_tier(tier)\n",
        "\n",
        "        end_flag = False\n",
        "        I = 0\n",
        "        R = 0\n",
        "        for name in names_annot:\n",
        "            tier = Annot_tg.get_tier_by_name(name)\n",
        "            if boundary_type in name:\n",
        "                for interval in tier.intervals:\n",
        "                    #print(method_boundaries)\n",
        "                    if interval.start_time < method_boundaries[0][0].start_time or interval.start_time > method_boundaries[-1][0].end_time:\n",
        "                        continue\n",
        "                    for mb in method_boundaries:\n",
        "                        if abs(interval.start_time - mb[0].start_time) < 0.01:\n",
        "                            mb[1] = '1'\n",
        "                        else:\n",
        "                            R += 1\n",
        "                        if mb[0].end_time == Method_tg.end_time:\n",
        "                            if abs(interval.end_time - mb[0].end_time) < 0.01:\n",
        "                                end_flag = True\n",
        "        \n",
        "        total = 0\n",
        "        hits = 0\n",
        "        for mb in method_boundaries:\n",
        "            #print(\"concordancia:\", nb)\n",
        "            if mb[1] == '1':\n",
        "                hits += 1\n",
        "            if mb[1] == '0':\n",
        "                I += 1\n",
        "            total += 1\n",
        "\n",
        "        if end_flag:\n",
        "            hits += 1\n",
        "        else:\n",
        "            I += 1\n",
        "\n",
        "        C = hits\n",
        "        SER = (I+R)/(C+R)\n",
        "\n",
        "        print(\"acertos:\", hits, '/', total, '=', hits/total)\n",
        "        print(\"métrica SER:\", '(I+R)/(C+R)', SER)\n",
        "\n",
        "        #Annot_tg.add_tier(agreement_tier)\n",
        "        #tgt.io.write_to_file(Annot_tg, annot_tg.replace(\".TextGrid\", \"_concat.TextGrid\"), format='long', encoding='utf-8')\n",
        "        return SER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tlC_UK2IpglI"
      },
      "outputs": [],
      "source": [
        "!chmod 755 -R ./automatic-segmentation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XRVYu_ZL9Jd"
      },
      "source": [
        "# Busca Exaustiva de Parâmetros com GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {rel_path_inq}; du -hs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSAfdA5m1SFy",
        "outputId": "8c0368b1-5cd7-4546-8edd-11f5ace671d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "914M\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wsvy7Zc_SIPr"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "class MeuSegmentadorBiron(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "\n",
        "    def __init__(self, p_segment_name='SP_D2_255',p_segment_number='1',\n",
        "                 p_db_threshold=37.0,\n",
        "                 p_window_size = 0.3,\n",
        "                 p_delta1 = 0.88,\n",
        "                 p_delta2 = 0.70,\n",
        "                 p_interval_size = 3,\n",
        "                 p_silence_threshold = 0.3,\n",
        "                 p_min_words_h2 = 10,\n",
        "                 p_layer = 'NTB'\n",
        "                 ):\n",
        "      \n",
        "        self.p_segment_name = p_segment_name\n",
        "        self.p_segment_number = p_segment_number\n",
        "        self.p_db_threshold = p_db_threshold\n",
        "        self.p_window_size = p_window_size\n",
        "        self.p_delta1 = p_delta1\n",
        "        self.p_delta2 = p_delta2\n",
        "        self.p_interval_size = p_interval_size\n",
        "        self.p_silence_threshold = p_silence_threshold\n",
        "        self.p_min_words_h2 = p_min_words_h2\n",
        "        self.p_layer = p_layer\n",
        "\n",
        "        self.hcode = hashlib.md5((p_segment_name+p_segment_number+str(random.randint(100000, 999999))).encode('utf-8')).hexdigest()\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def fit(self, X, y, **_k):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        name = self.p_segment_name\n",
        "        segment_number = self.p_segment_number\n",
        "        self.metric_ser_list = []\n",
        "\n",
        "        path = rel_path_inq + name + \"_\" + segment_number + \"/\"\n",
        "        # gamb aqui (para linux)\n",
        "        !cp -afxr {path} {self.hcode}\n",
        "        !cp -afxr {rel_path} {self.hcode+'_rel'}\n",
        "        self.mypath = self.hcode+'/'\n",
        "        self.mypath_rel = self.hcode+'_rel/'\n",
        "\n",
        "        audio_file = self.mypath + name + \"_clipped_\" + segment_number + \".wav\"\n",
        "        alignment_tg = self.mypath + name + \"_clipped_\" + segment_number + \".TextGrid\"\n",
        "        locs_file = self.mypath + \"locutores.txt\"\n",
        "        locs_file2 = self.mypath + \"locutores_palavras.txt\"\n",
        "        annot_tg = name + \"_segmentado/\" + name + \".TextGrid\"\n",
        "        text_align = self.mypath + \"locutores_palavras_align.txt\"\n",
        "        silences_file = self.mypath + \"new_wavs/silences.txt\"\n",
        "        method_tg = self.mypath + name + \"_clipped_\" + segment_number + \"_novo.TextGrid\"\n",
        "        wavs_path = self.mypath + \"new_wavs/\"\n",
        "        print(\"### Step A\")\n",
        "        Segmentation = AutomaticSegmentation(self.mypath, audio_file, locs_file)\n",
        "        print(\"### Step B\")\n",
        "        Segmentation.generate_words_file(locs_file)\n",
        "        print(\"### Step C\")\n",
        "        Segmentation.align(audio_file, text_align,self.mypath_rel)\n",
        "        print(\"### Step D\")\n",
        "        # 1 parâmetro: threshold para silêncio: 37.0 (valor em dB, positivo)\n",
        "        db_threshold = self.p_db_threshold\n",
        "        \n",
        "        Segmentation.find_silences(\"./\", self.mypath, wavs_path, 22050, 0.3, 10.0, 5.0, db_threshold, '', 1)\n",
        "        print(\"### Step E\")\n",
        "        # 6 parâmetros: tamanho da janela: 0.3                      (em s, deve ser positivo e não deve ser grande, talvez no max 1s)\n",
        "        #               threshold da 1a heurística (porcentagem\n",
        "        #                   da maior diferença de taxas de fala\n",
        "        #                   de janelas consecutivas para caracterizar\n",
        "        #                   DSR): 0.88                              (no intervalo [0, 1] e não muito pequeno, talvez no min 0.5 ou 0.6)\n",
        "        #               threshold da 2a heurística: 0.70            (no intervalo [0, 1], sempre abaixo do parâmetro anterior)\n",
        "        #               duração de segundos sem DSRs na primeira heuristica para contemplar a 2a: 3 (em s, positivo)\n",
        "        #               duração de silêncio para caracterizar pausa: 0.3 (em s, talvez no mínimo 0.15 e no max 1s)\n",
        "        #               palavras consecutivas sem DSR na 1a heuristica para contemplar a 2a: 10 (número inteiro talvez entre 5 e 20)\n",
        "        window_size = self.p_window_size\n",
        "        delta1 = self.p_delta1\n",
        "        delta2 = self.p_delta2\n",
        "        interval_size = self.p_interval_size\n",
        "        silence_threshold = self.p_silence_threshold\n",
        "        min_words_h2 = self.p_min_words_h2\n",
        "        Segmentation.find_boundaries(locs_file2, alignment_tg, silences_file, window_size, delta1, delta2, interval_size, silence_threshold, min_words_h2)\n",
        "        print(\"### Step F\")\n",
        "        self.metric_ser = Segmentation.ser(annot_tg, method_tg, self.p_layer)\n",
        "        print(\"### Step G\")\n",
        "        # gamb linux\n",
        "        !rm -rf {self.hcode}\n",
        "        !rm -rf {self.hcode+'_rel'}\n",
        "        print(\"### Step Fit Finish\")\n",
        "\n",
        "    def predict(self, X):\n",
        "         # Some code\n",
        "         print(self.metric_ser)\n",
        "         return [self.metric_ser]*len(X)\n",
        "\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_scoring(model, y_true,y_pred):\n",
        "  print('model',model)\n",
        "  print('segment_number',model.p_segment_number)\n",
        "  print('metric',model.metric_ser)\n",
        "  print('y_true',y_true)\n",
        "  print('y_pred',y_pred)\n",
        "  return model.metric_ser\n"
      ],
      "metadata": {
        "id": "98mG8_8pDDUC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "                 'p_segment_name': ['SP_D2_255'],\n",
        "                 'p_segment_number': ['1'],\n",
        "                 'p_db_threshold': [20.0, 25.0, 30.0, 35.0, 40.0],\n",
        "                 'p_window_size': [0.3],\n",
        "                 'p_delta1': [0.88],\n",
        "                 'p_delta2': [0.70],\n",
        "                 'p_interval_size': [3],\n",
        "                 'p_silence_threshold': [5],\n",
        "                 'p_min_words_h2': [10],\n",
        "                 'p_layer': ['NTB']\n",
        "}\n",
        "\n",
        "# parametros Sandra - D2\n",
        "# params = {\n",
        "#                  'p_segment_name': ['SP_D2_255'],\n",
        "#                  'p_segment_number': ['1'],\n",
        "#                  'p_db_threshold': [20.0, 25.0, 30.0, 35.0, 40.0],\n",
        "#                  'p_window_size': [0.25, 0.30, 0.35, 0.40, 0.45, 0.50],\n",
        "#                  'p_delta1': [0.6,0.65,0.7,0.75,0.8],\n",
        "#                  'p_delta2': [0.81,0.85,0.91,0.96],\n",
        "#                  'p_interval_size': [0.2, 1.4 , 2.6], \n",
        "#                  'p_silence_threshold': [5.3, 5.6, 5.9],\n",
        "#                  'p_min_words_h2': [4, 9],\n",
        "#                  'p_layer': ['NTB']\n",
        "# }\n",
        "\n",
        "def custom_cv_g(X,Y):\n",
        "  v = [0]*len(X)\n",
        "  yield v, v\n",
        "\n",
        "#### trick para usar o gridsearch do sklearn\n",
        "x = ['2','3','4','5','6','7','8']\n",
        "y = ['0','1','0','1','0','1','0']\n",
        "custom_cv = custom_cv_g(x,y)\n",
        "#### trick para usar o gridsearch do sklearn\n",
        "\n",
        "#### forcando mul\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "with parallel_backend('threading'):  # 'multiprocessing' / 'threading'\n",
        "    gs = GridSearchCV(MeuSegmentadorBiron(), param_grid=params, cv=custom_cv, scoring=my_scoring, refit=False, verbose=3, n_jobs=10)\n",
        "    gs.fit(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JOzpD5jB147r",
        "outputId": "d211e333-82c5-4489-af02-8aab1f58bfc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "esposa\n",
            "eSpohzA\n",
            "eu\n",
            "eU\n",
            "teria\n",
            "teRiA\n",
            "que\n",
            "ki\n",
            "fazer\n",
            "fazeRele\n",
            "elI\n",
            "que\n",
            "ki\n",
            "só\n",
            "soh\n",
            "\n",
            "poderia\n",
            "podeRi\n",
            "aceitar\n",
            "aseitaR\n",
            "se\n",
            "si\n",
            "fosse\n",
            "fosI\n",
            "possível\n",
            "posivEU\n",
            "levar\n",
            "levaR\n",
            "minha\n",
            "minh\n",
            "esposa\n",
            "eSpohzA\n",
            "tambem\n",
            "tambEm\n",
            "entao\n",
            "entaU\n",
            "ele\n",
            "elI\n",
            "disse\n",
            "dZisI\n",
            "que\n",
            "ki\n",
            "nessa\n",
            "nehsA\n",
            "circunstância\n",
            "siRkunStaNnsIA\n",
            "para\n",
            "paRA\n",
            "justificar\n",
            "zhuStSifikaR\n",
            "perante\n",
            "peRantS\n",
            "a\n",
            "a\n",
            "companhia\n",
            "kompanhia\n",
            "a\n",
            "a\n",
            "ida\n",
            "idA\n",
            "da\n",
            "da\n",
            "minha\n",
            "minhA\n",
            "minha\n",
            "minh\n",
            "esposa\n",
            "eSpohzA\n",
            "eu\n",
            "eU\n",
            "teria\n",
            "teRiA\n",
            "que\n",
            "ki\n",
            "fazer\n",
            "fazeR\n",
            "alguma\n",
            "aUgumA\n",
            "coisa\n",
            "kohIzA\n",
            "pela\n",
            "pehlA\n",
            "companhia\n",
            "kompanhia\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "aquiloalguma\n",
            "aUgumA\n",
            "coisa\n",
            "kohIzA\n",
            "pela\n",
            "pehlA\n",
            "companhia\n",
            "kompanhia\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "aquilo\n",
            "\n",
            "a\n",
            "aquilo\n",
            "akilU\n",
            "que\n",
            "ki\n",
            "o\n",
            "u\n",
            "professor\n",
            "pRofesoR\n",
            "sabe\n",
            "sabI\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "única\n",
            "unIkA\n",
            "e\n",
            "i\n",
            "exclusivamente\n",
            "eskluzivamentS\n",
            "é\n",
            "eh\n",
            "dar\n",
            "daR\n",
            "aula\n",
            "aUl\n",
            "entao\n",
            "entaU\n",
            "ele\n",
            "elI\n",
            "pediu\n",
            "pedZiU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "preparasse\n",
            "pRepaRasI\n",
            "uma\n",
            "umA\n",
            "aula\n",
            "aUlA\n",
            "para\n",
            "paR\n",
            "apresentar\n",
            "apRezentaR\n",
            "aos\n",
            "aUS\n",
            "passageiros\n",
            "pasazheiRUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "microfone\n",
            "mikRofonI\n",
            "de\n",
            "dZi\n",
            "de\n",
            "dZi\n",
            "bordo\n",
            "boRdU\n",
            "ahn\n",
            "aNn\n",
            "numa\n",
            "numA\n",
            "grande\n",
            "gRandZ\n",
            "altitude\n",
            "aUtSitudZ\n",
            "e\n",
            "i\n",
            "foi\n",
            "foI\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "de\n",
            "dZi\n",
            "oito\n",
            "oItU\n",
            "minutos\n",
            "minutUS\n",
            "nao\n",
            "naU\n",
            "mais\n",
            "maIS\n",
            "do\n",
            "du\n",
            "que\n",
            "ki\n",
            "isso\n",
            "isU\n",
            "sobre\n",
            "sobRI\n",
            "a\n",
            "a\n",
            "ocupaçao\n",
            "okupasaU\n",
            "amazônica\n",
            "amazonIkA\n",
            "do\n",
            "du\n",
            "interesse\n",
            "inteResI\n",
            "internacional\n",
            "inteRnasionaU\n",
            "pela\n",
            "pehlA\n",
            "regiao\n",
            "rezhiaU\n",
            "e\n",
            "i\n",
            "o\n",
            "u\n",
            "de\n",
            "dZi\n",
            "exótico\n",
            "ezohtSIkU\n",
            "é\n",
            "eh\n",
            "que\n",
            "ki\n",
            "acredito\n",
            "akRedZitU\n",
            "que\n",
            "ki\n",
            "poucas\n",
            "pohUkAS\n",
            "pessoas\n",
            "pesoaS\n",
            "tiveram\n",
            "tSiveRANU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "que\n",
            "ki\n",
            "tive\n",
            "tSivI\n",
            "de\n",
            "dZi\n",
            "dar\n",
            "daR\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "a\n",
            "a\n",
            "oito\n",
            "oItU\n",
            "mil\n",
            "miU\n",
            "metros\n",
            "metRUS\n",
            "de\n",
            "dZi\n",
            "altitude\n",
            "aUtSitudZ\n",
            "raridade\n",
            "raRidadZ\n",
            "né\n",
            "neh\n",
            "agora\n",
            "agohR\n",
            "outros\n",
            "outRUS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "vocês\n",
            "voseS\n",
            "se\n",
            "si\n",
            "utilizam\n",
            "utSilizANU\n",
            "de\n",
            "dZi\n",
            "alguns\n",
            "aUgunS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transportes\n",
            "tSRanSpoRtSIS\n",
            "agora\n",
            "agohR\n",
            "existem\n",
            "eziStEm\n",
            "outros\n",
            "outRUS\n",
            "que\n",
            "ki\n",
            "vocês\n",
            "voseS\n",
            "conhecem\n",
            "konhesEm\n",
            "né\n",
            "neh\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marítimas\n",
            "maRitSImAS\n",
            "sao\n",
            "saU\n",
            "extremamente\n",
            "estRemamentS\n",
            "limitadas\n",
            "limitadAS\n",
            "eu\n",
            "eU\n",
            "ahn\n",
            "aNn\n",
            "digamos\n",
            "dZigamUS\n",
            "assim\n",
            "asim\n",
            "como\n",
            "komU\n",
            "passeios\n",
            "paseiUS\n",
            "para\n",
            "paRA\n",
            "conheci\n",
            "konhesi\n",
            "mento\n",
            "mentU\n",
            "da\n",
            "da\n",
            "linha\n",
            "linhA\n",
            "náutica\n",
            "naUtSIkA\n",
            "o\n",
            "u\n",
            "oferta\n",
            "ofehRt\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "ministério\n",
            "miniStehRIU\n",
            "da\n",
            "da\n",
            "marinha\n",
            "maRinh\n",
            "algumas\n",
            "aUgumAS\n",
            "vezes\n",
            "vezIS\n",
            "eu\n",
            "eU\n",
            "tive\n",
            "tSivI\n",
            "a\n",
            "a\n",
            "oportunidade\n",
            "opoRtunidadZ\n",
            "da\n",
            "da\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "a\n",
            "a\n",
            "uma\n",
            "um\n",
            "incursao\n",
            "inkuRsaU\n",
            "pelo\n",
            "pelU\n",
            "mar\n",
            "maR\n",
            "muito\n",
            "muItU\n",
            "pequena\n",
            "pekenA\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "urbano\n",
            "uRbanU\n",
            "e\n",
            "i\n",
            "mesmo\n",
            "meSmU\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "suburbano\n",
            "subuRbanU\n",
            "e\n",
            "i\n",
            "às\n",
            "aS\n",
            "vezes\n",
            "vezIS\n",
            "até\n",
            "ateh\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "muitas\n",
            "muItAS\n",
            "vezes\n",
            "vezIS\n",
            "acabei\n",
            "akabeI\n",
            "me\n",
            "mi\n",
            "valendo\n",
            "valendU\n",
            "também\n",
            "tambem\n",
            "nessas\n",
            "nehsAS\n",
            "circunstâncias\n",
            "siRkunStaNnsIAS\n",
            "ahn\n",
            "aNn\n",
            "e\n",
            "i\n",
            "acredito\n",
            "akRedZitU\n",
            "até\n",
            "ateh\n",
            "que\n",
            "ki\n",
            "grande\n",
            "gRandZ\n",
            "parte\n",
            "paRtS\n",
            "de\n",
            "dZi\n",
            "minhas\n",
            "minhAS\n",
            "viagens\n",
            "viazhEnS\n",
            "pelo\n",
            "pelU\n",
            "estado\n",
            "eStadU\n",
            "de\n",
            "dZi\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "principalmente\n",
            "pRinsipaUmentS\n",
            "nos\n",
            "nuS\n",
            "pontos\n",
            "pontUS\n",
            "nao\n",
            "naU\n",
            "atendidos\n",
            "atendZidUS\n",
            "pelas\n",
            "pehlAS\n",
            "companhias\n",
            "kompanhiaS\n",
            "de\n",
            "dZi\n",
            "navegaçao\n",
            "navegasaU\n",
            "aérea\n",
            "aehREA\n",
            "foram\n",
            "foRANU\n",
            "feitos\n",
            "feItUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "detesto\n",
            "deteStU\n",
            "realmente\n",
            "reaUmentS\n",
            "apesar\n",
            "apezaR\n",
            "de\n",
            "dZi\n",
            "reconhecer\n",
            "rekonheseR\n",
            "que\n",
            "ki\n",
            "alguns\n",
            "aUgunS\n",
            "ahn\n",
            "aNn\n",
            "nos\n",
            "nuS\n",
            "ooferecem\n",
            "oofeResEm\n",
            "as\n",
            "aS\n",
            "condiçoes\n",
            "kondZisoIS\n",
            "extremamente\n",
            "estRemamentS\n",
            "favoráveis\n",
            "favoRavEIS\n",
            "de\n",
            "dZi\n",
            "viagem\n",
            "viazhEm\n",
            "mas\n",
            "maS\n",
            "ahn\n",
            "aNn\n",
            "sou\n",
            "soU\n",
            "um\n",
            "um\n",
            "indivíduo\n",
            "indZividUU\n",
            "muito\n",
            "muItU\n",
            "angustiado\n",
            "anguStSiadU\n",
            "pelo\n",
            "pelU\n",
            "fator\n",
            "fatoR\n",
            "tempo\n",
            "tempU\n",
            "muito\n",
            "muItU\n",
            "preocupado\n",
            "pReokupadU\n",
            "realmente\n",
            "reaUmentS\n",
            "com\n",
            "kom\n",
            "o\n",
            "u\n",
            "aproveitamento\n",
            "apRoveitamentU\n",
            "daquele\n",
            "dakelI\n",
            "tempo\n",
            "tempU\n",
            "nao\n",
            "naU\n",
            "tenho\n",
            "tenhU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "de\n",
            "dZi\n",
            "acordar\n",
            "akoRdaR\n",
            "e\n",
            "i\n",
            "me\n",
            "mi\n",
            "perguntar\n",
            "peRguntaR\n",
            "\n",
            "akilU\n",
            "que\n",
            "ki\n",
            "o\n",
            "u\n",
            "professor\n",
            "pRofesoR\n",
            "sabe\n",
            "sabI\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "única\n",
            "unIkA\n",
            "e\n",
            "i\n",
            "exclusivamente\n",
            "eskluzivamentS\n",
            "é\n",
            "eh\n",
            "dar\n",
            "daR\n",
            "aula\n",
            "aUl\n",
            "entao\n",
            "entaU\n",
            "ele\n",
            "elI\n",
            "pediu\n",
            "pedZiU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "preparasse\n",
            "pRepaRasI\n",
            "uma\n",
            "umA\n",
            "aula\n",
            "aUlA\n",
            "para\n",
            "paR\n",
            "apresentar\n",
            "apRezentaR\n",
            "aos\n",
            "aUS\n",
            "passageiros\n",
            "pasazheiRUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "microfone\n",
            "mikRofonI\n",
            "de\n",
            "dZi\n",
            "de\n",
            "dZi\n",
            "bordo\n",
            "boRdU\n",
            "ahn\n",
            "aNn\n",
            "numa\n",
            "numA\n",
            "grande\n",
            "gRandZ\n",
            "altitude\n",
            "aUtSitudZ\n",
            "e\n",
            "i\n",
            "foi\n",
            "foI\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "de\n",
            "dZi\n",
            "oito\n",
            "oItU\n",
            "minutos\n",
            "minutUS\n",
            "nao\n",
            "naU\n",
            "mais\n",
            "maIS\n",
            "do\n",
            "du\n",
            "que\n",
            "ki\n",
            "isso\n",
            "isU\n",
            "sobre\n",
            "sobRI\n",
            "a\n",
            "a\n",
            "ocupaçao\n",
            "okupasaU\n",
            "amazônica\n",
            "amazonIkA\n",
            "do\n",
            "du\n",
            "interesse\n",
            "inteResI\n",
            "internacional\n",
            "inteRnasionaU\n",
            "pela\n",
            "pehlA\n",
            "regiao\n",
            "rezhiaU\n",
            "e\n",
            "i\n",
            "o\n",
            "u\n",
            "de\n",
            "dZi\n",
            "exótico\n",
            "ezohtSIkU\n",
            "é\n",
            "eh\n",
            "que\n",
            "ki\n",
            "acredito\n",
            "akRedZitU\n",
            "que\n",
            "ki\n",
            "poucas\n",
            "pohUkAS\n",
            "pessoas\n",
            "pesoaS\n",
            "tiveram\n",
            "tSiveRANU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "que\n",
            "ki\n",
            "tive\n",
            "tSivI\n",
            "de\n",
            "dZi\n",
            "dar\n",
            "daR\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "a\n",
            "a\n",
            "oito\n",
            "oItU\n",
            "mil\n",
            "miU\n",
            "metros\n",
            "metRUS\n",
            "de\n",
            "dZi\n",
            "altitude\n",
            "aUtSitudZ\n",
            "raridade\n",
            "raRidadZ\n",
            "né\n",
            "neh\n",
            "agora\n",
            "agohR\n",
            "outros\n",
            "outRUS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "vocês\n",
            "voseS\n",
            "se\n",
            "si\n",
            "utilizam\n",
            "utSilizANU\n",
            "de\n",
            "dZi\n",
            "alguns\n",
            "aUgunS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transportes\n",
            "tSRanSpoRtSIS\n",
            "agora\n",
            "agohR\n",
            "existem\n",
            "eziStEm\n",
            "outros\n",
            "outRUS\n",
            "que\n",
            "ki\n",
            "vocês\n",
            "voseS\n",
            "conhecem\n",
            "konhesEm\n",
            "né\n",
            "neh\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marítimas\n",
            "maRitSImAS\n",
            "sao\n",
            "saU\n",
            "extremamente\n",
            "estRemamentS\n",
            "limitadas\n",
            "limitadAS\n",
            "eu\n",
            "eU\n",
            "ahn\n",
            "aNn\n",
            "digamos\n",
            "dZigamUS\n",
            "assim\n",
            "asim\n",
            "como\n",
            "komU\n",
            "passeios\n",
            "paseiUS\n",
            "para\n",
            "paRA\n",
            "conheci\n",
            "konhesi\n",
            "mento\n",
            "mentU\n",
            "da\n",
            "da\n",
            "linha\n",
            "linhA\n",
            "náutica\n",
            "naUtSIkA\n",
            "o\n",
            "u\n",
            "oferta\n",
            "ofehRt\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "ministério\n",
            "miniStehRIU\n",
            "da\n",
            "da\n",
            "marinha\n",
            "maRinh\n",
            "algumas\n",
            "aUgumAS\n",
            "vezes\n",
            "vezIS\n",
            "eu\n",
            "eU\n",
            "tive\n",
            "tSivI\n",
            "a\n",
            "a\n",
            "oportunidade\n",
            "opoRtunidadZ\n",
            "da\n",
            "da\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "a\n",
            "a\n",
            "uma\n",
            "um\n",
            "incursao\n",
            "inkuRsaU\n",
            "pelo\n",
            "pelU\n",
            "mar\n",
            "maR\n",
            "muito\n",
            "muItU\n",
            "pequena\n",
            "pekenA\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "urbano\n",
            "uRbanU\n",
            "e\n",
            "i\n",
            "mesmo\n",
            "meSmU\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "suburbano\n",
            "subuRbanUaquilo\n",
            "akilU\n",
            "que\n",
            "ki\n",
            "o\n",
            "u\n",
            "professor\n",
            "pRofesoR\n",
            "sabe\n",
            "sabI\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "única\n",
            "unIkA\n",
            "e\n",
            "i\n",
            "exclusivamente\n",
            "eskluzivamentS\n",
            "é\n",
            "eh\n",
            "dar\n",
            "daR\n",
            "aula\n",
            "aUl\n",
            "entao\n",
            "entaU\n",
            "ele\n",
            "elI\n",
            "pediu\n",
            "pedZiU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "preparasse\n",
            "pRepaRasI\n",
            "uma\n",
            "umA\n",
            "aula\n",
            "aUlA\n",
            "para\n",
            "paR\n",
            "apresentar\n",
            "apRezentaR\n",
            "aos\n",
            "aUS\n",
            "passageiros\n",
            "pasazheiRUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "microfone\n",
            "mikRofonI\n",
            "de\n",
            "dZi\n",
            "de\n",
            "dZi\n",
            "bordo\n",
            "boRdU\n",
            "ahn\n",
            "aNn\n",
            "numa\n",
            "numA\n",
            "grande\n",
            "gRandZ\n",
            "altitude\n",
            "aUtSitudZ\n",
            "e\n",
            "i\n",
            "foi\n",
            "foI\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "de\n",
            "dZi\n",
            "oito\n",
            "oItU\n",
            "minutos\n",
            "minutUS\n",
            "nao\n",
            "naU\n",
            "mais\n",
            "maIS\n",
            "do\n",
            "du\n",
            "que\n",
            "ki\n",
            "isso\n",
            "isU\n",
            "sobre\n",
            "sobRI\n",
            "a\n",
            "a\n",
            "ocupaçao\n",
            "okupasaU\n",
            "amazônica\n",
            "amazonIkA\n",
            "do\n",
            "du\n",
            "interesse\n",
            "inteResI\n",
            "internacional\n",
            "inteRnasionaU\n",
            "pela\n",
            "pehlA\n",
            "regiao\n",
            "rezhiaU\n",
            "e\n",
            "i\n",
            "o\n",
            "u\n",
            "de\n",
            "dZi\n",
            "exótico\n",
            "ezohtSIkU\n",
            "é\n",
            "eh\n",
            "que\n",
            "ki\n",
            "acredito\n",
            "akRedZitU\n",
            "que\n",
            "ki\n",
            "poucas\n",
            "e\n",
            "i\n",
            "às\n",
            "aS\n",
            "vezes\n",
            "vezIS\n",
            "até\n",
            "ateh\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "muitas\n",
            "muItAS\n",
            "vezes\n",
            "vezIS\n",
            "acabei\n",
            "akabeI\n",
            "me\n",
            "mi\n",
            "valendo\n",
            "valendU\n",
            "também\n",
            "tambem\n",
            "nessas\n",
            "nehsAS\n",
            "circunstâncias\n",
            "siRkunStaNnsIAS\n",
            "ahn\n",
            "aNn\n",
            "e\n",
            "i\n",
            "acredito\n",
            "akRedZitU\n",
            "até\n",
            "ateh\n",
            "que\n",
            "ki\n",
            "\n",
            "pohUkAS\n",
            "pessoas\n",
            "grande\n",
            "gRandZ\n",
            "parte\n",
            "paRtS\n",
            "de\n",
            "dZi\n",
            "minhas\n",
            "minhAS\n",
            "viagens\n",
            "viazhEnS\n",
            "pelo\n",
            "pelU\n",
            "estado\n",
            "eStadU\n",
            "de\n",
            "dZi\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "principalmente\n",
            "pRinsipaUmentS\n",
            "nos\n",
            "nuS\n",
            "pontos\n",
            "pontUS\n",
            "nao\n",
            "naU\n",
            "atendidos\n",
            "atendZidUS\n",
            "pelas\n",
            "pehlAS\n",
            "companhias\n",
            "kompanhiaS\n",
            "de\n",
            "dZi\n",
            "navegaçao\n",
            "navegasaU\n",
            "aérea\n",
            "aehREA\n",
            "foram\n",
            "foRANU\n",
            "feitos\n",
            "feItUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "detesto\n",
            "deteStU\n",
            "realmente\n",
            "reaUmentS\n",
            "apesar\n",
            "apezaR\n",
            "de\n",
            "dZi\n",
            "reconhecer\n",
            "rekonheseR\n",
            "que\n",
            "ki\n",
            "alguns\n",
            "aUgunS\n",
            "ahn\n",
            "aNn\n",
            "nos\n",
            "pesoaS\n",
            "tiveramnuS\n",
            "tSiveRANU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "que\n",
            "ki\n",
            "tive\n",
            "tSivI\n",
            "de\n",
            "dZi\n",
            "dar\n",
            "daR\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "a\n",
            "a\n",
            "\n",
            "ooferecem\n",
            "oofeResEm\n",
            "as\n",
            "aS\n",
            "condiçoes\n",
            "kondZisoIS\n",
            "extremamente\n",
            "estRemamentS\n",
            "favoráveis\n",
            "favoRavEIS\n",
            "de\n",
            "dZi\n",
            "viagem\n",
            "viazhEm\n",
            "mas\n",
            "maS\n",
            "ahn\n",
            "aNn\n",
            "sou\n",
            "soU\n",
            "um\n",
            "um\n",
            "indivíduo\n",
            "indZividUU\n",
            "muito\n",
            "muItU\n",
            "angustiado\n",
            "anguStSiadU\n",
            "pelo\n",
            "pelU\n",
            "fator\n",
            "fatoR\n",
            "tempo\n",
            "tempU\n",
            "muito\n",
            "muItU\n",
            "preocupado\n",
            "pReokupadU\n",
            "realmente\n",
            "reaUmentS\n",
            "com\n",
            "kom\n",
            "o\n",
            "u\n",
            "aproveitamento\n",
            "apRoveitamentU\n",
            "daquele\n",
            "dakelI\n",
            "tempo\n",
            "tempU\n",
            "nao\n",
            "naU\n",
            "tenho\n",
            "tenhU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "de\n",
            "dZi\n",
            "acordar\n",
            "akoRdaR\n",
            "e\n",
            "i\n",
            "me\n",
            "mi\n",
            "perguntar\n",
            "peRguntaR\n",
            "o\n",
            "u\n",
            "queoito\n",
            "oItU\n",
            "mil\n",
            "miU\n",
            "metros\n",
            "metRUS\n",
            "de\n",
            "dZi\n",
            "altitude\n",
            "aUtSitudZ\n",
            "raridade\n",
            "raRidadZ\n",
            "né\n",
            "neh\n",
            "agora\n",
            "agohR\n",
            "outros\n",
            "outRUS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "vocês\n",
            "voseS\n",
            "se\n",
            "si\n",
            "utilizam\n",
            "utSilizANU\n",
            "de\n",
            "dZi\n",
            "alguns\n",
            "aUgunS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transportes\n",
            "tSRanSpoRtSIS\n",
            "agora\n",
            "agohR\n",
            "existem\n",
            "eziStEm\n",
            "outros\n",
            "outRUS\n",
            "que\n",
            "ki\n",
            "vocês\n",
            "voseS\n",
            "conhecem\n",
            "konhesEm\n",
            "né\n",
            "neh\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marítimas\n",
            "maRitSImAS\n",
            "sao\n",
            "saU\n",
            "extremamente\n",
            "estRemamentS\n",
            "limitadas\n",
            "limitadAS\n",
            "eu\n",
            "eU\n",
            "ahn\n",
            "aNn\n",
            "digamos\n",
            "dZigamUS\n",
            "assim\n",
            "asim\n",
            "como\n",
            "komU\n",
            "passeios\n",
            "paseiUS\n",
            "para\n",
            "paRA\n",
            "conheci\n",
            "konhesi\n",
            "mento\n",
            "mentU\n",
            "da\n",
            "da\n",
            "linha\n",
            "linhA\n",
            "náutica\n",
            "naUtSIkA\n",
            "o\n",
            "u\n",
            "oferta\n",
            "ofehRt\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "ministério\n",
            "miniStehRIU\n",
            "da\n",
            "da\n",
            "marinha\n",
            "maRinh\n",
            "algumas\n",
            "aUgumAS\n",
            "vezes\n",
            "vezIS\n",
            "eu\n",
            "eU\n",
            "tive\n",
            "tSivI\n",
            "a\n",
            "a\n",
            "oportunidade\n",
            "opoRtunidadZ\n",
            "da\n",
            "da\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "a\n",
            "a\n",
            "uma\n",
            "um\n",
            "incursao\n",
            "inkuRsaU\n",
            "pelo\n",
            "pelU\n",
            "mar\n",
            "maR\n",
            "muito\n",
            "muItU\n",
            "pequena\n",
            "pekenA\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "urbano\n",
            "uRbanU\n",
            "e\n",
            "i\n",
            "mesmo\n",
            "meSmU\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "suburbano\n",
            "subuRbanU\n",
            "e\n",
            "i\n",
            "às\n",
            "aS\n",
            "vezes\n",
            "vezIS\n",
            "até\n",
            "ateh\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "muitas\n",
            "muItAS\n",
            "vezes\n",
            "vezIS\n",
            "acabei\n",
            "akabeI\n",
            "me\n",
            "mi\n",
            "valendo\n",
            "valendU\n",
            "também\n",
            "tambem\n",
            "nessas\n",
            "nehsAS\n",
            "circunstâncias\n",
            "siRkunStaNnsIAS\n",
            "ahn\n",
            "aNn\n",
            "e\n",
            "i\n",
            "acredito\n",
            "akRedZitU\n",
            "até\n",
            "ateh\n",
            "que\n",
            "ki\n",
            "grande\n",
            "gRandZ\n",
            "parte\n",
            "paRtS\n",
            "de\n",
            "dZi\n",
            "minhas\n",
            "minhAS\n",
            "viagens\n",
            "viazhEnS\n",
            "pelo\n",
            "pelU\n",
            "estado\n",
            "eStadU\n",
            "de\n",
            "dZi\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "principalmente\n",
            "pRinsipaUmentS\n",
            "nos\n",
            "nuS\n",
            "pontos\n",
            "pontUS\n",
            "nao\n",
            "naU\n",
            "atendidos\n",
            "atendZidUS\n",
            "pelas\n",
            "pehlAS\n",
            "companhias\n",
            "kompanhiaS\n",
            "de\n",
            "dZi\n",
            "navegaçao\n",
            "navegasaU\n",
            "aérea\n",
            "aehREA\n",
            "foram\n",
            "foRANU\n",
            "feitos\n",
            "feItUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "detesto\n",
            "deteStU\n",
            "realmente\n",
            "reaUmentS\n",
            "apesar\n",
            "apezaR\n",
            "de\n",
            "dZi\n",
            "reconhecer\n",
            "rekonheseR\n",
            "que\n",
            "ki\n",
            "alguns\n",
            "aUgunS\n",
            "ahn\n",
            "aNn\n",
            "nos\n",
            "nuS\n",
            "ooferecem\n",
            "oofeResEm\n",
            "as\n",
            "aS\n",
            "condiçoes\n",
            "kondZisoIS\n",
            "extremamente\n",
            "estRemamentS\n",
            "favoráveis\n",
            "favoRavEIS\n",
            "de\n",
            "dZi\n",
            "viagem\n",
            "viazhEm\n",
            "mas\n",
            "maS\n",
            "ahn\n",
            "aNn\n",
            "sou\n",
            "soU\n",
            "um\n",
            "um\n",
            "indivíduo\n",
            "indZividUU\n",
            "muito\n",
            "muItU\n",
            "angustiado\n",
            "anguStSiadU\n",
            "pelo\n",
            "pelU\n",
            "fator\n",
            "fatoR\n",
            "tempo\n",
            "tempU\n",
            "muito\n",
            "muItU\n",
            "preocupado\n",
            "pReokupadU\n",
            "realmente\n",
            "reaUmentS\n",
            "com\n",
            "kom\n",
            "o\n",
            "u\n",
            "aproveitamento\n",
            "apRoveitamentU\n",
            "daquele\n",
            "dakelI\n",
            "tempo\n",
            "tempU\n",
            "nao\n",
            "naU\n",
            "tenho\n",
            "tenhU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "de\n",
            "dZi\n",
            "acordar\n",
            "akoRdaR\n",
            "e\n",
            "i\n",
            "me\n",
            "mi\n",
            "perguntar\n",
            "peRguntaR\n",
            "o\n",
            "uakilU\n",
            "que\n",
            "ki\n",
            "o\n",
            "u\n",
            "professor\n",
            "pRofesoR\n",
            "sabe\n",
            "sabI\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "única\n",
            "unIkA\n",
            "e\n",
            "i\n",
            "exclusivamente\n",
            "eskluzivamentS\n",
            "é\n",
            "eh\n",
            "dar\n",
            "daR\n",
            "aula\n",
            "aUl\n",
            "entao\n",
            "entaU\n",
            "ele\n",
            "elI\n",
            "pediu\n",
            "pedZiU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "preparasse\n",
            "pRepaRasI\n",
            "uma\n",
            "umA\n",
            "aula\n",
            "aUlA\n",
            "para\n",
            "paR\n",
            "apresentar\n",
            "apRezentaR\n",
            "aos\n",
            "aUS\n",
            "passageiros\n",
            "pasazheiRUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "microfone\n",
            "mikRofonI\n",
            "de\n",
            "dZi\n",
            "de\n",
            "dZi\n",
            "bordo\n",
            "boRdU\n",
            "ahn\n",
            "aNn\n",
            "numa\n",
            "numA\n",
            "grande\n",
            "gRandZ\n",
            "altitude\n",
            "aUtSitudZ\n",
            "e\n",
            "i\n",
            "foi\n",
            "foI\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "de\n",
            "dZi\n",
            "oito\n",
            "oItU\n",
            "minutos\n",
            "minutUS\n",
            "nao\n",
            "naU\n",
            "mais\n",
            "maIS\n",
            "do\n",
            "du\n",
            "que\n",
            "ki\n",
            "isso\n",
            "isU\n",
            "sobre\n",
            "sobRI\n",
            "a\n",
            "a\n",
            "ocupaçao\n",
            "okupasaU\n",
            "amazônica\n",
            "amazonIkA\n",
            "do\n",
            "du\n",
            "interesse\n",
            "inteResI\n",
            "internacional\n",
            "inteRnasionaU\n",
            "pela\n",
            "pehlA\n",
            "regiao\n",
            "rezhiaU\n",
            "e\n",
            "i\n",
            "o\n",
            "u\n",
            "de\n",
            "dZi\n",
            "exótico\n",
            "ezohtSIkU\n",
            "é\n",
            "eh\n",
            "que\n",
            "ki\n",
            "acredito\n",
            "akRedZitU\n",
            "que\n",
            "ki\n",
            "poucas\n",
            "pohUkAS\n",
            "pessoas\n",
            "pesoaS\n",
            "tiveram\n",
            "tSiveRANU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "que\n",
            "ki\n",
            "tive\n",
            "tSivI\n",
            "de\n",
            "dZi\n",
            "dar\n",
            "daR\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "a\n",
            "a\n",
            "oito\n",
            "oItU\n",
            "mil\n",
            "miU\n",
            "metros\n",
            "metRUS\n",
            "de\n",
            "dZi\n",
            "altitude\n",
            "aUtSitudZ\n",
            "raridade\n",
            "raRidadZ\n",
            "né\n",
            "neh\n",
            "agora\n",
            "agohR\n",
            "outros\n",
            "outRUS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "vocês\n",
            "voseS\n",
            "se\n",
            "si\n",
            "utilizam\n",
            "utSilizANU\n",
            "de\n",
            "dZi\n",
            "alguns\n",
            "aUgunS\n",
            "meios\n",
            "meIUS\n",
            "de\n",
            "dZi\n",
            "transportes\n",
            "tSRanSpoRtSIS\n",
            "agora\n",
            "agohR\n",
            "existem\n",
            "eziStEm\n",
            "outros\n",
            "outRUS\n",
            "que\n",
            "ki\n",
            "vocês\n",
            "voseS\n",
            "conhecem\n",
            "konhesEm\n",
            "né\n",
            "neh\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marítimas\n",
            "maRitSImAS\n",
            "sao\n",
            "saU\n",
            "extremamente\n",
            "estRemamentS\n",
            "limitadas\n",
            "limitadAS\n",
            "eu\n",
            "eU\n",
            "ahn\n",
            "aNn\n",
            "digamos\n",
            "dZigamUS\n",
            "assim\n",
            "asim\n",
            "como\n",
            "komU\n",
            "passeios\n",
            "paseiUS\n",
            "para\n",
            "paRA\n",
            "conheci\n",
            "konhesi\n",
            "mento\n",
            "mentU\n",
            "da\n",
            "da\n",
            "linha\n",
            "linhA\n",
            "náutica\n",
            "naUtSIkA\n",
            "o\n",
            "u\n",
            "oferta\n",
            "ofehRt\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "ministério\n",
            "miniStehRIU\n",
            "da\n",
            "da\n",
            "marinha\n",
            "maRinh\n",
            "akilU\n",
            "que\n",
            "ki\n",
            "o\n",
            "u\n",
            "professor\n",
            "pRofesoR\n",
            "sabe\n",
            "sabI\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "única\n",
            "unIkA\n",
            "e\n",
            "i\n",
            "exclusivamente\n",
            "eskluzivamentS\n",
            "é\n",
            "eh\n",
            "dar\n",
            "daR\n",
            "aula\n",
            "aUl\n",
            "entao\n",
            "entaU\n",
            "ele\n",
            "elI\n",
            "pediu\n",
            "pedZiU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "preparasse\n",
            "pRepaRasI\n",
            "uma\n",
            "umA\n",
            "aula\n",
            "aUlA\n",
            "para\n",
            "paR\n",
            "apresentar\n",
            "apRezentaR\n",
            "aos\n",
            "aUS\n",
            "passageiros\n",
            "pasazheiRUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "microfone\n",
            "mikRofonI\n",
            "de\n",
            "dZi\n",
            "de\n",
            "dZi\n",
            "bordo\n",
            "boRdU\n",
            "ahn\n",
            "aNn\n",
            "numa\n",
            "numA\n",
            "grande\n",
            "gRandZ\n",
            "altitude\n",
            "aUtSitudZ\n",
            "e\n",
            "i\n",
            "foi\n",
            "foI\n",
            "uma\n",
            "umA\n",
            "palestra\n",
            "palehStRA\n",
            "de\n",
            "dZi\n",
            "oito\n",
            "oItU\n",
            "algumas\n",
            "aUgumAS\n",
            "vezes\n",
            "vezIS\n",
            "eu\n",
            "eU\n",
            "tive\n",
            "tSivI\n",
            "a\n",
            "a\n",
            "oportunidade\n",
            "opoRtunidadZ\n",
            "da\n",
            "da\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "a\n",
            "a\n",
            "uma\n",
            "um\n",
            "incursao\n",
            "inkuRsaU\n",
            "pelo\n",
            "pelU\n",
            "mar\n",
            "maR\n",
            "muito\n",
            "muItU\n",
            "pequena\n",
            "pekenA\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "urbano\n",
            "uRbanU\n",
            "e\n",
            "i\n",
            "mesmo\n",
            "meSmU\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "suburbano\n",
            "subuRbanU\n",
            "e\n",
            "i\n",
            "às\n",
            "aS\n",
            "vezes\n",
            "vezIS\n",
            "até\n",
            "ateh\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "muitas\n",
            "muItAS\n",
            "vezes\n",
            "vezIS\n",
            "acabei\n",
            "\n",
            "minutos\n",
            "akabeIminutUS\n",
            "\n",
            "nao\n",
            "menaU\n",
            "\n",
            "mais\n",
            "mimaIS\n",
            "\n",
            "do\n",
            "valendodu\n",
            "\n",
            "que\n",
            "valendUki\n",
            "\n",
            "isso\n",
            "tambémisU\n",
            "\n",
            "sobre\n",
            "sobRItambem\n",
            "a\n",
            "\n",
            "anessas\n",
            "ocupaçao\n",
            "\n",
            "okupasaUnehsAS\n",
            "amazônica\n",
            "\n",
            "amazonIkAcircunstâncias\n",
            "do\n",
            "\n",
            "dusiRkunStaNnsIAS\n",
            "interesse\n",
            "\n",
            "inteResIahn\n",
            "internacional\n",
            "\n",
            "inteRnasionaUaNn\n",
            "pela\n",
            "e\n",
            "\n",
            "i\n",
            "acreditopehlA\n",
            "\n",
            "regiaoakRedZitU\n",
            "rezhiaU\n",
            "\n",
            "eaté\n",
            "i\n",
            "\n",
            "oateh\n",
            "u\n",
            "\n",
            "deque\n",
            "dZi\n",
            "\n",
            "exóticoki\n",
            "ezohtSIkU\n",
            "\n",
            "égrande\n",
            "eh\n",
            "gRandZ\n",
            "parte\n",
            "\n",
            "quepaRtS\n",
            "ki\n",
            "\n",
            "acreditode\n",
            "dZi\n",
            "minhas\n",
            "\n",
            "akRedZitU\n",
            "minhAS\n",
            "viagens\n",
            "viazhEnS\n",
            "pelo\n",
            "pelU\n",
            "estado\n",
            "eStadU\n",
            "de\n",
            "dZi\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "principalmente\n",
            "pRinsipaUmentS\n",
            "nos\n",
            "nuS\n",
            "pontos\n",
            "pontUS\n",
            "nao\n",
            "naU\n",
            "atendidos\n",
            "atendZidUS\n",
            "pelas\n",
            "pehlAS\n",
            "companhias\n",
            "kompanhiaS\n",
            "de\n",
            "dZi\n",
            "navegaçao\n",
            "navegasaU\n",
            "aérea\n",
            "aehREA\n",
            "foram\n",
            "foRANU\n",
            "feitos\n",
            "feItUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "duque\n",
            "\n",
            "do\n",
            "du\n",
            "ônibuski\n",
            "\n",
            "poucasonIbUS\n",
            "pohUkAS\n",
            "\n",
            "pessoasinterestadual\n",
            "pesoaS\n",
            "\n",
            "tiveram\n",
            "inteReStaduaUtSiveRANU\n",
            "\n",
            "que\n",
            "ki\n",
            "o\n",
            "euu\n",
            "\n",
            "privilégio\n",
            "eUpRivilehzhIU\n",
            "\n",
            "que\n",
            "detestoki\n",
            "\n",
            "tive\n",
            "deteStU\n",
            "realmente\n",
            "reaUmentStSivI\n",
            "\n",
            "de\n",
            "apesardZi\n",
            "\n",
            "dar\n",
            "apezaRdaR\n",
            "\n",
            "uma\n",
            "deumA\n",
            "\n",
            "palestra\n",
            "dZipalehStRA\n",
            "\n",
            "a\n",
            "reconhecera\n",
            "\n",
            "rekonheseR\n",
            "que\n",
            "kioito\n",
            "\n",
            "oItU\n",
            "algunsmil\n",
            "\n",
            "miU\n",
            "aUgunSmetros\n",
            "\n",
            "metRUS\n",
            "deahn\n",
            "dZi\n",
            "\n",
            "altitudeaNn\n",
            "aUtSitudZ\n",
            "\n",
            "raridadenos\n",
            "raRidadZ\n",
            "\n",
            "nénuS\n",
            "neh\n",
            "\n",
            "agoraooferecem\n",
            "agohR\n",
            "\n",
            "outrosoofeResEm\n",
            "outRUS\n",
            "as\n",
            "aS\n",
            "condiçoes\n",
            "meios\n",
            "kondZisoIS\n",
            "extremamente\n",
            "\n",
            "meIUS\n",
            "estRemamentSde\n",
            "dZi\n",
            "favoráveis\n",
            "favoRavEIS\n",
            "de\n",
            "dZi\n",
            "viagem\n",
            "viazhEm\n",
            "mas\n",
            "maS\n",
            "ahn\n",
            "aNn\n",
            "sou\n",
            "soU\n",
            "um\n",
            "um\n",
            "indivíduo\n",
            "indZividUU\n",
            "muito\n",
            "muItU\n",
            "angustiado\n",
            "anguStSiadU\n",
            "pelo\n",
            "pelU\n",
            "fator\n",
            "fatoR\n",
            "tempo\n",
            "tempU\n",
            "muito\n",
            "muItU\n",
            "preocupado\n",
            "pReokupadU\n",
            "realmente\n",
            "reaUmentS\n",
            "com\n",
            "kom\n",
            "o\n",
            "u\n",
            "aproveitamento\n",
            "apRoveitamentU\n",
            "daquele\n",
            "dakelI\n",
            "tempo\n",
            "tempU\n",
            "nao\n",
            "naU\n",
            "tenho\n",
            "tenhU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "\n",
            "transporte\n",
            "detRanSpoRtS\n",
            "\n",
            "vocês\n",
            "dZivoseS\n",
            "\n",
            "se\n",
            "acordarsi\n",
            "\n",
            "utilizam\n",
            "akoRdaRutSilizANU\n",
            "\n",
            "de\n",
            "edZi\n",
            "\n",
            "alguns\n",
            "iaUgunS\n",
            "\n",
            "meios\n",
            "memeIUS\n",
            "\n",
            "de\n",
            "midZi\n",
            "\n",
            "transportes\n",
            "perguntartSRanSpoRtSIS\n",
            "\n",
            "agora\n",
            "peRguntaRagohR\n",
            "\n",
            "existem\n",
            "oeziStEm\n",
            "\n",
            "outros\n",
            "uoutRUS\n",
            "\n",
            "queque\n",
            "\n",
            "ki\n",
            "vocês\n",
            "voseS\n",
            "conhecem\n",
            "konhesEm\n",
            "né\n",
            "neh\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marítimas\n",
            "maRitSImAS\n",
            "sao\n",
            "saU\n",
            "extremamente\n",
            "estRemamentS\n",
            "limitadas\n",
            "limitadAS\n",
            "eu\n",
            "eU\n",
            "ahn\n",
            "aNn\n",
            "digamos\n",
            "dZigamUS\n",
            "assim\n",
            "asim\n",
            "como\n",
            "komU\n",
            "passeios\n",
            "paseiUS\n",
            "para\n",
            "paRA\n",
            "conheci\n",
            "konhesi\n",
            "mento\n",
            "mentU\n",
            "da\n",
            "da\n",
            "linha\n",
            "linhA\n",
            "náutica\n",
            "naUtSIkA\n",
            "o\n",
            "u\n",
            "oferta\n",
            "ofehRt\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "ministério\n",
            "miniStehRIU\n",
            "da\n",
            "da\n",
            "marinha\n",
            "maRinh\n",
            "algumas\n",
            "aUgumAS\n",
            "vezes\n",
            "vezIS\n",
            "eu\n",
            "eU\n",
            "tive\n",
            "tSivI\n",
            "a\n",
            "a\n",
            "oportunidade\n",
            "opoRtunidadZ\n",
            "da\n",
            "da\n",
            "fazer\n",
            "fazeR\n",
            "ahn\n",
            "aNn\n",
            "a\n",
            "a\n",
            "uma\n",
            "um\n",
            "incursao\n",
            "inkuRsaU\n",
            "pelo\n",
            "pelU\n",
            "mar\n",
            "maR\n",
            "muito\n",
            "muItU\n",
            "pequena\n",
            "pekenA\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "urbano\n",
            "uRbanU\n",
            "e\n",
            "i\n",
            "mesmo\n",
            "meSmU\n",
            "o\n",
            "u\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "suburbano\n",
            "subuRbanU\n",
            "e\n",
            "i\n",
            "às\n",
            "aS\n",
            "vezes\n",
            "vezIS\n",
            "até\n",
            "ateh\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "muitas\n",
            "muItAS\n",
            "vezes\n",
            "vezIS\n",
            "acabei\n",
            "akabeI\n",
            "me\n",
            "mi\n",
            "valendo\n",
            "valendU\n",
            "também\n",
            "tambem\n",
            "nessas\n",
            "nehsAS\n",
            "circunstâncias\n",
            "siRkunStaNnsIAS\n",
            "ahn\n",
            "aNn\n",
            "e\n",
            "i\n",
            "acredito\n",
            "akRedZitU\n",
            "até\n",
            "ateh\n",
            "que\n",
            "ki\n",
            "grande\n",
            "gRandZ\n",
            "parte\n",
            "paRtS\n",
            "de\n",
            "dZi\n",
            "minhas\n",
            "minhAS\n",
            "viagens\n",
            "viazhEnS\n",
            "pelo\n",
            "pelU\n",
            "estado\n",
            "eStadU\n",
            "de\n",
            "dZi\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "principalmente\n",
            "pRinsipaUmentS\n",
            "nos\n",
            "nuS\n",
            "pontos\n",
            "pontUS\n",
            "nao\n",
            "naU\n",
            "atendidos\n",
            "atendZidUS\n",
            "pelas\n",
            "pehlAS\n",
            "companhias\n",
            "kompanhiaS\n",
            "de\n",
            "dZi\n",
            "navegaçao\n",
            "navegasaU\n",
            "aérea\n",
            "aehREA\n",
            "foram\n",
            "foRANU\n",
            "feitos\n",
            "feItUS\n",
            "através\n",
            "atRavehS\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "do\n",
            "du\n",
            "ônibus\n",
            "onIbUS\n",
            "interestadual\n",
            "inteReStaduaU\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "detesto\n",
            "deteStU\n",
            "realmente\n",
            "reaUmentS\n",
            "apesar\n",
            "apezaR\n",
            "de\n",
            "dZi\n",
            "reconhecer\n",
            "rekonheseR\n",
            "que\n",
            "ki\n",
            "alguns\n",
            "aUgunS\n",
            "ahn\n",
            "aNn\n",
            "nos\n",
            "nuS\n",
            "ooferecem\n",
            "oofeResEm\n",
            "as\n",
            "aS\n",
            "condiçoes\n",
            "kondZisoIS\n",
            "extremamente\n",
            "estRemamentS\n",
            "favoráveis\n",
            "favoRavEIS\n",
            "de\n",
            "dZi\n",
            "viagem\n",
            "viazhEm\n",
            "mas\n",
            "maS\n",
            "ahn\n",
            "aNn\n",
            "sou\n",
            "soU\n",
            "um\n",
            "um\n",
            "indivíduo\n",
            "indZividUU\n",
            "muito\n",
            "muItU\n",
            "angustiado\n",
            "anguStSiadU\n",
            "pelo\n",
            "pelU\n",
            "fator\n",
            "fatoR\n",
            "tempo\n",
            "tempU\n",
            "muito\n",
            "muItU\n",
            "preocupado\n",
            "pReokupadU\n",
            "realmente\n",
            "reaUmentS\n",
            "com\n",
            "kom\n",
            "o\n",
            "u\n",
            "aproveitamento\n",
            "apRoveitamentU\n",
            "daquele\n",
            "dakelI\n",
            "tempo\n",
            "tempU\n",
            "nao\n",
            "naU\n",
            "tenho\n",
            "tenhU\n",
            "o\n",
            "u\n",
            "privilégio\n",
            "pRivilehzhIU\n",
            "de\n",
            "dZi\n",
            "acordar\n",
            "akoRdaR\n",
            "e\n",
            "i\n",
            "me\n",
            "mi\n",
            "perguntar\n",
            "peRguntaR\n",
            "o\n",
            "u\n",
            "queo\n",
            "u\n",
            "que\n",
            "ki\n",
            "farei\n",
            "faReI\n",
            "no\n",
            "nu\n",
            "dia\n",
            "dZiA\n",
            "de\n",
            "dZi\n",
            "hoje\n",
            "ozhI\n",
            "né\n",
            "neh\n",
            "entao\n",
            "entaU\n",
            "esta\n",
            "ehSt\n",
            "impossibilidade\n",
            "imposibilidadZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "valer\n",
            "valeR\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "tempo\n",
            "tempU\n",
            "faz\n",
            "faS\n",
            "com\n",
            "kom\n",
            "que\n",
            "ki\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "pelo\n",
            "pelU\n",
            "ônibus\n",
            "onIbUS\n",
            "sejam\n",
            "sezhANU\n",
            "sempre\n",
            "sempRI\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marcadas\n",
            "maRkadAS\n",
            "por\n",
            "poR\n",
            "uma\n",
            "um\n",
            "angustia\n",
            "anguStSiA\n",
            "de\n",
            "dZi\n",
            "chegar\n",
            "shegaR\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "uma\n",
            "umA\n",
            "preocupaçao\n",
            "pReokupasaU\n",
            "muito\n",
            "muItU\n",
            "grande\n",
            "gRandZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "me\n",
            "mi\n",
            "ver\n",
            "veR\n",
            "assim\n",
            "asim\n",
            "rapidamente\n",
            "rapidamentS\n",
            "desin\n",
            "dezin\n",
            "cumbido\n",
            "kumbidU\n",
            "daquela\n",
            "dakelA\n",
            "tarefa\n",
            "taRehfA\n",
            "o\n",
            "u\n",
            "r\n",
            "r\n",
            "deve\n",
            "devI\n",
            "ter\n",
            "teR\n",
            "andado\n",
            "andadU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "por\n",
            "poR\n",
            "aí\n",
            "ai\n",
            "bastante\n",
            "baStantS\n",
            "também\n",
            "tambem\n",
            "de\n",
            "dZi\n",
            "navio\n",
            "naviU\n",
            "nao\n",
            "naU\n",
            "sei\n",
            "seI\n",
            "éh\n",
            "eh\n",
            "eu\n",
            "eU\n",
            "realmente\n",
            "reaUmentS\n",
            "tenho\n",
            "tenhU\n",
            "parentes\n",
            "paRentSIS\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "na\n",
            "na\n",
            "guanabara\n",
            "gUanabaR\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ando\n",
            "andU\n",
            "muito\n",
            "muItU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "hoje\n",
            "ozhI\n",
            "nem\n",
            "nem\n",
            "tanto\n",
            "tantU\n",
            "mas\n",
            "maS\n",
            "há\n",
            "a\n",
            "algum\n",
            "aUgum\n",
            "tempo\n",
            "tempU\n",
            "atrás\n",
            "atRaS\n",
            "andava\n",
            "andavA\n",
            "bastante\n",
            "baStantS\n",
            "era\n",
            "ehRA\n",
            "raro\n",
            "raRU\n",
            "o\n",
            "u\n",
            "mês\n",
            "meS\n",
            "ou\n",
            "oU\n",
            "cada\n",
            "kadA\n",
            "dois\n",
            "doIS\n",
            "meses\n",
            "mezIS\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "nao\n",
            "naU\n",
            "ia\n",
            "iA\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "ou\n",
            "oU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "mesmo\n",
            "meSmU\n",
            "até\n",
            "ateh\n",
            "a\n",
            "a\n",
            "guanabara\n",
            "gUanabaRA\n",
            "eu\n",
            "eU\n",
            "viajei\n",
            "viazheI\n",
            "muito\n",
            "muItU\n",
            "daquele\n",
            "dakelI\n",
            "de\n",
            "dZi\n",
            "santa\n",
            "santA\n",
            "cruz\n",
            "kRuS\n",
            "aquele\n",
            "akelI\n",
            "noturno\n",
            "notuRnU\n",
            "né\n",
            "neh\n",
            "várias\n",
            "vaRIAS\n",
            "vezes\n",
            "vezIS\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "apenas\n",
            "apenAS\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "quando\n",
            "kUandU\n",
            "eu\n",
            "eU\n",
            "estava\n",
            "eStavA\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "lá\n",
            "la\n",
            "dá\n",
            "da\n",
            "na\n",
            "na\n",
            "praça\n",
            "pRasA\n",
            "xv\n",
            "shv\n",
            "até\n",
            "ateh\n",
            "paquetá\n",
            "paketa\n",
            "fui\n",
            "fuI\n",
            "muito\n",
            "muItU\n",
            "a\n",
            "a\n",
            "paquetá\n",
            "paketa\n",
            "de\n",
            "dZi\n",
            "naquelas\n",
            "nakelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "que\n",
            "ki\n",
            "lembram\n",
            "lembRANU\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "do\n",
            "du\n",
            "mississipi\n",
            "misisipi\n",
            "né\n",
            "neh\n",
            "com\n",
            "kom\n",
            "aquelas\n",
            "akelAS\n",
            "rodas\n",
            "rohdAS\n",
            "laterais\n",
            "lateRaIS\n",
            "em\n",
            "em\n",
            "vi\n",
            "vi\n",
            "mas\n",
            "maS\n",
            "nao\n",
            "naU\n",
            "por\n",
            "poR\n",
            "sinal\n",
            "sinaU\n",
            "que\n",
            "ki\n",
            "é\n",
            "eh\n",
            "uma\n",
            "umA\n",
            "delícia\n",
            "delisI\n",
            "aquilo\n",
            "akilU\n",
            "sabe\n",
            "sabI\n",
            "aquilo\n",
            "akilU\n",
            "me\n",
            "mi\n",
            "dá\n",
            "da\n",
            "uma\n",
            "umA\n",
            "saudade\n",
            "saudadZ\n",
            "imensa\n",
            "imensA\n",
            "o\n",
            "u\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "jovem\n",
            "zhovEm\n",
            "porque\n",
            "poRki\n",
            "todas\n",
            "todAS\n",
            "as\n",
            "aS\n",
            "férias\n",
            "fehRIAS\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "em\n",
            "em\n",
            "paquetá\n",
            "paketa\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "e\n",
            "i\n",
            "lá\n",
            "la\n",
            "você\n",
            "vose\n",
            "devia\n",
            "deviA\n",
            "dizer\n",
            "dZizeR\n",
            "no\n",
            "nu\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "agora\n",
            "agohRA\n",
            "e\n",
            "i\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "as\n",
            "aS\n",
            "conduçoes\n",
            "kondusoIS\n",
            "aqui\n",
            "aki\n",
            "em\n",
            "em\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "por\n",
            "poR\n",
            "exemplo\n",
            "ezemplU\n",
            "\n",
            "ki\n",
            "farei\n",
            "faReI\n",
            "no\n",
            "nu\n",
            "dia\n",
            "dZiA\n",
            "de\n",
            "dZi\n",
            "hoje\n",
            "ozhI\n",
            "né\n",
            "neh\n",
            "entao\n",
            "entaU\n",
            "esta\n",
            "ehSt\n",
            "impossibilidade\n",
            "imposibilidadZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "valer\n",
            "valeR\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "tempo\n",
            "tempU\n",
            "faz\n",
            "faS\n",
            "com\n",
            "kom\n",
            "que\n",
            "ki\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "pelo\n",
            "pelU\n",
            "ônibus\n",
            "onIbUS\n",
            "sejam\n",
            "sezhANU\n",
            "sempre\n",
            "sempRI\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marcadas\n",
            "maRkadAS\n",
            "por\n",
            "poR\n",
            "uma\n",
            "um\n",
            "angustia\n",
            "anguStSiA\n",
            "de\n",
            "dZi\n",
            "chegar\n",
            "shegaR\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "uma\n",
            "umA\n",
            "preocupaçao\n",
            "pReokupasaU\n",
            "muito\n",
            "muItU\n",
            "grande\n",
            "gRandZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "me\n",
            "mi\n",
            "ver\n",
            "veR\n",
            "assim\n",
            "asim\n",
            "rapidamente\n",
            "rapidamentS\n",
            "desin\n",
            "dezin\n",
            "cumbido\n",
            "kumbidU\n",
            "daquela\n",
            "dakelA\n",
            "tarefa\n",
            "taRehfA\n",
            "o\n",
            "u\n",
            "r\n",
            "r\n",
            "deve\n",
            "devI\n",
            "ter\n",
            "teR\n",
            "andado\n",
            "andadU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "por\n",
            "poR\n",
            "aí\n",
            "ai\n",
            "bastante\n",
            "baStantS\n",
            "também\n",
            "tambem\n",
            "de\n",
            "dZi\n",
            "navio\n",
            "naviU\n",
            "nao\n",
            "naU\n",
            "sei\n",
            "seI\n",
            "éh\n",
            "eh\n",
            "eu\n",
            "eU\n",
            "realmente\n",
            "reaUmentS\n",
            "tenho\n",
            "tenhU\n",
            "parentes\n",
            "paRentSIS\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "na\n",
            "na\n",
            "que\n",
            "ki\n",
            "farei\n",
            "faReI\n",
            "no\n",
            "nu\n",
            "aligner:dia\n",
            " HTKdZiA\n",
            "\n",
            "hmmdefs:de\n",
            " FalsedZi\n",
            "\n",
            "hoje\n",
            "\n",
            "guanabara\n",
            "gUanabaR\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ando\n",
            "andU\n",
            "muito\n",
            "muItU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUSozhI\n",
            "né\n",
            "neh\n",
            "entao\n",
            "entaU\n",
            "\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "hoje\n",
            "ozhI\n",
            "nem\n",
            "nem\n",
            "tanto\n",
            "tantU\n",
            "mas\n",
            "maS\n",
            "há\n",
            "a\n",
            "algum\n",
            "aUgum\n",
            "tempo\n",
            "tempU\n",
            "atrás\n",
            "atRaS\n",
            "andava\n",
            "andavA\n",
            "bastante\n",
            "baStantS\n",
            "era\n",
            "ehRA\n",
            "raro\n",
            "raRU\n",
            "o\n",
            "u\n",
            "mês\n",
            "meS\n",
            "ou\n",
            "oU\n",
            "cada\n",
            "kadA\n",
            "dois\n",
            "doIS\n",
            "meses\n",
            "mezIS\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "esta\n",
            "ehSt\n",
            "impossibilidade\n",
            "imposibilidadZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "valer\n",
            "valeR\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "tempo\n",
            "tempU\n",
            "faz\n",
            "faS\n",
            "com\n",
            "kom\n",
            "que\n",
            "ki\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "pelo\n",
            "pelU\n",
            "ônibus\n",
            "onIbUS\n",
            "sejam\n",
            "sezhANU\n",
            "sempre\n",
            "sempRI\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marcadas\n",
            "maRkadAS\n",
            "por\n",
            "poR\n",
            "umanao\n",
            "um\n",
            "\n",
            "naU\n",
            "angustia\n",
            "anguStSiA\n",
            "de\n",
            "dZiia\n",
            "iA\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "ou\n",
            "oU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "mesmo\n",
            "meSmU\n",
            "até\n",
            "ateh\n",
            "a\n",
            "a\n",
            "guanabara\n",
            "gUanabaRA\n",
            "eu\n",
            "eU\n",
            "viajei\n",
            "viazheI\n",
            "muito\n",
            "muItU\n",
            "daquele\n",
            "dakelI\n",
            "de\n",
            "dZi\n",
            "santa\n",
            "santA\n",
            "cruz\n",
            "kRuS\n",
            "aquele\n",
            "akelI\n",
            "noturno\n",
            "notuRnU\n",
            "né\n",
            "neh\n",
            "várias\n",
            "vaRIAS\n",
            "vezes\n",
            "vezIS\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "apenas\n",
            "apenAS\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "chegar\n",
            "shegaR\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "uma\n",
            "umA\n",
            "preocupaçao\n",
            "pReokupasaU\n",
            "muito\n",
            "muItU\n",
            "grande\n",
            "gRandZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "me\n",
            "mi\n",
            "ver\n",
            "veR\n",
            "assim\n",
            "asim\n",
            "rapidamente\n",
            "rapidamentS\n",
            "desin\n",
            "dezin\n",
            "cumbido\n",
            "kumbidU\n",
            "daquela\n",
            "dakelA\n",
            "tarefa\n",
            "taRehfA\n",
            "o\n",
            "u\n",
            "r\n",
            "r\n",
            "deve\n",
            "devI\n",
            "ter\n",
            "teR\n",
            "andado\n",
            "andadU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "por\n",
            "poR\n",
            "aí\n",
            "ai\n",
            "bastante\n",
            "baStantS\n",
            "também\n",
            "tambem\n",
            "de\n",
            "dZi\n",
            "navio\n",
            "naviU\n",
            "nao\n",
            "naU\n",
            "sei\n",
            "seI\n",
            "éh\n",
            "eh\n",
            "eu\n",
            "eU\n",
            "da\n",
            "\n",
            "realmente\n",
            "da\n",
            "cantareirareaUmentS\n",
            "tenho\n",
            "kantaRehIRA\n",
            "quando\n",
            "kUandU\n",
            "eu\n",
            "eU\n",
            "estava\n",
            "eStavA\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "lá\n",
            "la\n",
            "dá\n",
            "da\n",
            "na\n",
            "na\n",
            "praça\n",
            "pRasA\n",
            "xv\n",
            "shv\n",
            "até\n",
            "ateh\n",
            "paquetá\n",
            "paketa\n",
            "fui\n",
            "fuI\n",
            "muito\n",
            "muItU\n",
            "a\n",
            "a\n",
            "paquetá\n",
            "paketa\n",
            "de\n",
            "dZi\n",
            "naquelas\n",
            "nakelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "que\n",
            "ki\n",
            "lembram\n",
            "lembRANU\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "do\n",
            "du\n",
            "mississipi\n",
            "misisipi\n",
            "né\n",
            "tenhU\n",
            "parentes\n",
            "paRentSIS\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "na\n",
            "na\n",
            "guanabara\n",
            "gUanabaR\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ando\n",
            "andU\n",
            "muito\n",
            "muItU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "hoje\n",
            "ozhI\n",
            "nem\n",
            "nem\n",
            "tanto\n",
            "tantU\n",
            "mas\n",
            "maS\n",
            "há\n",
            "a\n",
            "algum\n",
            "aUgum\n",
            "tempo\n",
            "tempU\n",
            "atrás\n",
            "atRaS\n",
            "andava\n",
            "andavA\n",
            "bastante\n",
            "baStantS\n",
            "era\n",
            "ehRA\n",
            "raro\n",
            "raRU\n",
            "o\n",
            "u\n",
            "mês\n",
            "meS\n",
            "ou\n",
            "oU\n",
            "cada\n",
            "kadA\n",
            "\n",
            "neh\n",
            "com\n",
            "kom\n",
            "aquelas\n",
            "akelAS\n",
            "rodas\n",
            "rohdAS\n",
            "laterais\n",
            "lateRaIS\n",
            "em\n",
            "em\n",
            "vi\n",
            "vi\n",
            "mas\n",
            "maS\n",
            "nao\n",
            "naU\n",
            "por\n",
            "poR\n",
            "sinal\n",
            "sinaU\n",
            "que\n",
            "ki\n",
            "é\n",
            "eh\n",
            "uma\n",
            "umA\n",
            "delícia\n",
            "delisI\n",
            "aquilo\n",
            "akilU\n",
            "sabe\n",
            "sabI\n",
            "aquilo\n",
            "akilU\n",
            "me\n",
            "mi\n",
            "dá\n",
            "da\n",
            "uma\n",
            "umA\n",
            "saudade\n",
            "saudadZ\n",
            "imensa\n",
            "imensA\n",
            "o\n",
            "u\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "jovem\n",
            "zhovEm\n",
            "porque\n",
            "poRki\n",
            "todas\n",
            "todAS\n",
            "as\n",
            "aS\n",
            "férias\n",
            "fehRIAS\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "em\n",
            "em\n",
            "paquetá\n",
            "paketa\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "e\n",
            "i\n",
            "lá\n",
            "la\n",
            "você\n",
            "vose\n",
            "devia\n",
            "deviA\n",
            "dizer\n",
            "dZizeR\n",
            "no\n",
            "nu\n",
            "tempo\n",
            "tempUdois\n",
            "\n",
            "que\n",
            "doIS\n",
            "meses\n",
            "mezIS\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "nao\n",
            "naU\n",
            "ia\n",
            "iA\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "ou\n",
            "oU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "mesmo\n",
            "meSmU\n",
            "até\n",
            "ateh\n",
            "a\n",
            "a\n",
            "guanabara\n",
            "gUanabaRA\n",
            "eu\n",
            "eU\n",
            "viajei\n",
            "viazheI\n",
            "muito\n",
            "muItU\n",
            "daquele\n",
            "dakelI\n",
            "de\n",
            "dZi\n",
            "santa\n",
            "santA\n",
            "cruz\n",
            "kRuS\n",
            "aquele\n",
            "akelI\n",
            "noturno\n",
            "notuRnU\n",
            "né\n",
            "neh\n",
            "várias\n",
            "vaRIAS\n",
            "vezes\n",
            "vezIS\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "apenas\n",
            "apenAS\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "quando\n",
            "kUandU\n",
            "eu\n",
            "eU\n",
            "estava\n",
            "eStavA\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "lá\n",
            "la\n",
            "dá\n",
            "da\n",
            "na\n",
            "na\n",
            "praça\n",
            "pRasA\n",
            "xv\n",
            "shv\n",
            "até\n",
            "ateh\n",
            "paquetá\n",
            "paketa\n",
            "fui\n",
            "fuI\n",
            "muito\n",
            "muItU\n",
            "a\n",
            "amais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "agora\n",
            "agohRA\n",
            "e\n",
            "i\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "as\n",
            "aS\n",
            "conduçoes\n",
            "kondusoIS\n",
            "aqui\n",
            "aki\n",
            "em\n",
            "em\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "por\n",
            "poR\n",
            "exemplo\n",
            "ezemplU\n",
            "\n",
            "paquetá\n",
            "paketa\n",
            "de\n",
            "dZi\n",
            "naquelas\n",
            "nakelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "que\n",
            "ki\n",
            "lembram\n",
            "lembRANU\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "do\n",
            "du\n",
            "mississipi\n",
            "misisipi\n",
            "né\n",
            "neh\n",
            "com\n",
            "kom\n",
            "aquelas\n",
            "akelAS\n",
            "rodas\n",
            "rohdAS\n",
            "laterais\n",
            "lateRaIS\n",
            "em\n",
            "em\n",
            "vi\n",
            "vi\n",
            "mas\n",
            "maS\n",
            "nao\n",
            "naU\n",
            "por\n",
            "poR\n",
            "sinal\n",
            "sinaU\n",
            "que\n",
            "ki\n",
            "é\n",
            "eh\n",
            "uma\n",
            "umA\n",
            "delícia\n",
            "delisI\n",
            "aquilo\n",
            "akilU\n",
            "sabe\n",
            "sabI\n",
            "aquilo\n",
            "akilU\n",
            "me\n",
            "mi\n",
            "dá\n",
            "da\n",
            "uma\n",
            "umA\n",
            "saudade\n",
            "saudadZ\n",
            "imensa\n",
            "imensA\n",
            "o\n",
            "u\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "jovem\n",
            "zhovEm\n",
            "porque\n",
            "poRki\n",
            "todas\n",
            "todAS\n",
            "as\n",
            "aS\n",
            "férias\n",
            "fehRIAS\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "em\n",
            "em\n",
            "paquetá\n",
            "paketa\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "e\n",
            "i\n",
            "lá\n",
            "la\n",
            "você\n",
            "vose\n",
            "devia\n",
            "deviA\n",
            "dizer\n",
            "dZizeR\n",
            "no\n",
            "nu\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "agora\n",
            "agohRA\n",
            "e\n",
            "i\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "as\n",
            "aS\n",
            "conduçoes\n",
            "kondusoIS\n",
            "aqui\n",
            "aki\n",
            "em\n",
            "em\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "por\n",
            "poR\n",
            "exemplo\n",
            "ezemplU\n",
            "ki\n",
            "farei\n",
            "faReI\n",
            "no\n",
            "nu\n",
            "dia\n",
            "dZiA\n",
            "de\n",
            "dZi\n",
            "hoje\n",
            "ozhI\n",
            "né\n",
            "neh\n",
            "entao\n",
            "entaU\n",
            "esta\n",
            "ehSt\n",
            "impossibilidade\n",
            "imposibilidadZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "valer\n",
            "valeR\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "tempo\n",
            "tempU\n",
            "faz\n",
            "faS\n",
            "com\n",
            "kom\n",
            "que\n",
            "ki\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "pelo\n",
            "pelU\n",
            "ônibus\n",
            "onIbUS\n",
            "sejam\n",
            "sezhANU\n",
            "sempre\n",
            "sempRI\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marcadas\n",
            "maRkadAS\n",
            "por\n",
            "poR\n",
            "uma\n",
            "um\n",
            "angustia\n",
            "anguStSiA\n",
            "de\n",
            "dZi\n",
            "chegar\n",
            "shegaR\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "uma\n",
            "umA\n",
            "preocupaçao\n",
            "pReokupasaU\n",
            "muito\n",
            "muItU\n",
            "grande\n",
            "gRandZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "me\n",
            "mi\n",
            "ver\n",
            "veR\n",
            "assim\n",
            "asim\n",
            "rapidamente\n",
            "rapidamentS\n",
            "desin\n",
            "dezin\n",
            "cumbido\n",
            "kumbidU\n",
            "daquela\n",
            "dakelA\n",
            "tarefa\n",
            "taRehfA\n",
            "o\n",
            "u\n",
            "r\n",
            "r\n",
            "deve\n",
            "devI\n",
            "ter\n",
            "teR\n",
            "andado\n",
            "andadU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "por\n",
            "poR\n",
            "aí\n",
            "ai\n",
            "bastante\n",
            "baStantS\n",
            "também\n",
            "tambem\n",
            "de\n",
            "dZi\n",
            "navio\n",
            "naviU\n",
            "nao\n",
            "naU\n",
            "sei\n",
            "seI\n",
            "éh\n",
            "eh\n",
            "eu\n",
            "eU\n",
            "realmente\n",
            "reaUmentS\n",
            "tenho\n",
            "tenhU\n",
            "parentes\n",
            "paRentSIS\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "na\n",
            "na\n",
            "guanabara\n",
            "gUanabaR\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ando\n",
            "andU\n",
            "muito\n",
            "muItU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "hoje\n",
            "ozhI\n",
            "nem\n",
            "nem\n",
            "tanto\n",
            "tantU\n",
            "mas\n",
            "maS\n",
            "há\n",
            "a\n",
            "algum\n",
            "aUgum\n",
            "tempo\n",
            "tempU\n",
            "atrás\n",
            "atRaS\n",
            "andava\n",
            "andavA\n",
            "bastante\n",
            "baStantS\n",
            "era\n",
            "ehRA\n",
            "raro\n",
            "raRU\n",
            "o\n",
            "u\n",
            "mês\n",
            "meS\n",
            "ou\n",
            "oU\n",
            "cada\n",
            "kadA\n",
            "dois\n",
            "doIS\n",
            "meses\n",
            "mezIS\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "nao\n",
            "naU\n",
            "ia\n",
            "iA\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "ki\n",
            "farei\n",
            "faReI\n",
            "no\n",
            "nu\n",
            "dia\n",
            "dZiA\n",
            "de\n",
            "dZi\n",
            "hoje\n",
            "ozhI\n",
            "né\n",
            "neh\n",
            "entao\n",
            "entaU\n",
            "esta\n",
            "ehSt\n",
            "impossibilidade\n",
            "imposibilidadZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "valer\n",
            "valeR\n",
            "assim\n",
            "asim\n",
            "do\n",
            "du\n",
            "tempo\n",
            "tempU\n",
            "faz\n",
            "faS\n",
            "com\n",
            "kom\n",
            "que\n",
            "ki\n",
            "as\n",
            "aS\n",
            "minhas\n",
            "minhAS\n",
            "experiências\n",
            "espeRiensIAS\n",
            "de\n",
            "dZi\n",
            "transporte\n",
            "tRanSpoRtS\n",
            "pelo\n",
            "pelU\n",
            "ônibus\n",
            "onIbUS\n",
            "sejam\n",
            "sezhANU\n",
            "sempre\n",
            "sempRI\n",
            "experiências\n",
            "espeRiensIAS\n",
            "marcadas\n",
            "maRkadAS\n",
            "por\n",
            "poR\n",
            "uma\n",
            "um\n",
            "angustia\n",
            "anguStSiA\n",
            "de\n",
            "dZi\n",
            "chegar\n",
            "shegaR\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "uma\n",
            "umA\n",
            "preocupaçao\n",
            "pReokupasaU\n",
            "muito\n",
            "muItU\n",
            "grande\n",
            "gRandZ\n",
            "de\n",
            "dZi\n",
            "me\n",
            "mi\n",
            "me\n",
            "mi\n",
            "ver\n",
            "veR\n",
            "assim\n",
            "asim\n",
            "rapidamente\n",
            "rapidamentS\n",
            "desin\n",
            "dezin\n",
            "cumbido\n",
            "kumbidU\n",
            "daquela\n",
            "dakelA\n",
            "tarefa\n",
            "taRehfA\n",
            "o\n",
            "u\n",
            "r\n",
            "r\n",
            "deve\n",
            "devI\n",
            "ter\n",
            "teR\n",
            "andado\n",
            "andadU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "por\n",
            "poR\n",
            "aí\n",
            "ai\n",
            "bastante\n",
            "baStantS\n",
            "também\n",
            "tambem\n",
            "de\n",
            "dZi\n",
            "navio\n",
            "naviU\n",
            "nao\n",
            "naU\n",
            "sei\n",
            "seI\n",
            "éh\n",
            "eh\n",
            "eu\n",
            "eU\n",
            "realmente\n",
            "reaUmentS\n",
            "tenho\n",
            "tenhU\n",
            "parentes\n",
            "paRentSISaligner: HTK\n",
            "hmmdefs: False\n",
            "\n",
            "ou\n",
            "oU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "mesmo\n",
            "meSmU\n",
            "até\n",
            "ateh\n",
            "a\n",
            "a\n",
            "guanabara\n",
            "gUanabaRA\n",
            "eu\n",
            "eU\n",
            "viajei\n",
            "viazheI\n",
            "muito\n",
            "muItU\n",
            "daquele\n",
            "dakelI\n",
            "de\n",
            "dZi\n",
            "santa\n",
            "santA\n",
            "cruz\n",
            "kRuS\n",
            "aquele\n",
            "akelI\n",
            "noturno\n",
            "notuRnU\n",
            "né\n",
            "neh\n",
            "várias\n",
            "vaRIAS\n",
            "vezes\n",
            "vezIS\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "apenas\n",
            "apenAS\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "quando\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "na\n",
            "na\n",
            "guanabara\n",
            "gUanabaR\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ando\n",
            "andU\n",
            "muito\n",
            "muItU\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "hoje\n",
            "ozhI\n",
            "nem\n",
            "nem\n",
            "tanto\n",
            "tantU\n",
            "mas\n",
            "maS\n",
            "há\n",
            "a\n",
            "algum\n",
            "aUgum\n",
            "tempo\n",
            "tempU\n",
            "atrás\n",
            "atRaS\n",
            "andava\n",
            "andavA\n",
            "bastante\n",
            "baStantS\n",
            "era\n",
            "ehRA\n",
            "raro\n",
            "raRU\n",
            "o\n",
            "u\n",
            "mês\n",
            "meS\n",
            "ou\n",
            "oU\n",
            "cada\n",
            "kadA\n",
            "dois\n",
            "doIS\n",
            "meses\n",
            "mezIS\n",
            "que\n",
            "ki\n",
            "eu\n",
            "eU\n",
            "nao\n",
            "naU\n",
            "ia\n",
            "iA\n",
            "de\n",
            "dZi\n",
            "ônibus\n",
            "onIbUS\n",
            "ou\n",
            "oU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "mesmo\n",
            "meSmU\n",
            "até\n",
            "atehaligner: HTK\n",
            "hmmdefs: False\n",
            "\n",
            "a\n",
            "a\n",
            "guanabara\n",
            "gUanabaRA\n",
            "eu\n",
            "eU\n",
            "viajei\n",
            "viazheI\n",
            "muito\n",
            "muItU\n",
            "daquele\n",
            "dakelI\n",
            "de\n",
            "dZi\n",
            "santa\n",
            "santA\n",
            "cruz\n",
            "kRuS\n",
            "aquele\n",
            "akelI\n",
            "kUandU\n",
            "eu\n",
            "eU\n",
            "estava\n",
            "eStavA\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "lá\n",
            "la\n",
            "dá\n",
            "da\n",
            "na\n",
            "na\n",
            "praça\n",
            "pRasA\n",
            "xv\n",
            "shv\n",
            "até\n",
            "ateh\n",
            "paquetá\n",
            "paketa\n",
            "fui\n",
            "fuI\n",
            "muito\n",
            "muItU\n",
            "a\n",
            "a\n",
            "paquetá\n",
            "paketa\n",
            "de\n",
            "dZi\n",
            "naquelas\n",
            "nakelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "que\n",
            "ki\n",
            "lembram\n",
            "lembRANU\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "do\n",
            "du\n",
            "mississipi\n",
            "misisipi\n",
            "né\n",
            "neh\n",
            "com\n",
            "kom\n",
            "aquelas\n",
            "akelAS\n",
            "rodas\n",
            "rohdAS\n",
            "laterais\n",
            "lateRaIS\n",
            "em\n",
            "em\n",
            "vi\n",
            "vi\n",
            "mas\n",
            "maS\n",
            "noturno\n",
            "notuRnU\n",
            "né\n",
            "neh\n",
            "várias\n",
            "vaRIAS\n",
            "vezes\n",
            "vezIS\n",
            "e\n",
            "i\n",
            "por\n",
            "poR\n",
            "mar\n",
            "maR\n",
            "apenas\n",
            "apenAS\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "quando\n",
            "kUandU\n",
            "eu\n",
            "eU\n",
            "estava\n",
            "eStavA\n",
            "no\n",
            "nu\n",
            "rio\n",
            "riU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "lá\n",
            "la\n",
            "dá\n",
            "da\n",
            "na\n",
            "na\n",
            "praça\n",
            "pRasA\n",
            "xv\n",
            "shv\n",
            "até\n",
            "ateh\n",
            "paquetá\n",
            "paketa\n",
            "fui\n",
            "fuI\n",
            "muito\n",
            "muItU\n",
            "a\n",
            "a\n",
            "paquetá\n",
            "paketa\n",
            "de\n",
            "dZi\n",
            "naquelas\n",
            "nakelAS\n",
            "barcas\n",
            "baRkAS\n",
            "da\n",
            "da\n",
            "cantareira\n",
            "kantaRehIRA\n",
            "que\n",
            "ki\n",
            "lembram\n",
            "lembRANU\n",
            "aquelas\n",
            "akelAS\n",
            "barcas\n",
            "baRkAS\n",
            "do\n",
            "du\n",
            "mississipi\n",
            "misisipi\n",
            "né\n",
            "neh\n",
            "com\n",
            "nao\n",
            "naU\n",
            "por\n",
            "poR\n",
            "sinal\n",
            "sinaU\n",
            "que\n",
            "ki\n",
            "é\n",
            "eh\n",
            "uma\n",
            "umA\n",
            "delícia\n",
            "delisI\n",
            "aquilo\n",
            "akilU\n",
            "sabe\n",
            "sabI\n",
            "aquilo\n",
            "akilU\n",
            "me\n",
            "mi\n",
            "dá\n",
            "da\n",
            "uma\n",
            "umA\n",
            "saudade\n",
            "saudadZ\n",
            "imensa\n",
            "imensA\n",
            "o\n",
            "u\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "jovem\n",
            "zhovEm\n",
            "porque\n",
            "poRki\n",
            "todas\n",
            "todAS\n",
            "as\n",
            "aS\n",
            "férias\n",
            "fehRIAS\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "em\n",
            "em\n",
            "paquetá\n",
            "paketa\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "e\n",
            "i\n",
            "lá\n",
            "la\n",
            "você\n",
            "vose\n",
            "devia\n",
            "deviA\n",
            "dizer\n",
            "dZizeR\n",
            "no\n",
            "nu\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "agora\n",
            "agohRA\n",
            "kom\n",
            "aquelas\n",
            "akelAS\n",
            "rodas\n",
            "rohdAS\n",
            "laterais\n",
            "lateRaIS\n",
            "em\n",
            "em\n",
            "vi\n",
            "vi\n",
            "mas\n",
            "maS\n",
            "nao\n",
            "naU\n",
            "por\n",
            "poR\n",
            "sinal\n",
            "sinaU\n",
            "que\n",
            "ki\n",
            "é\n",
            "eh\n",
            "uma\n",
            "umA\n",
            "delícia\n",
            "delisI\n",
            "aquilo\n",
            "akilU\n",
            "sabe\n",
            "sabI\n",
            "aquilo\n",
            "akilU\n",
            "me\n",
            "mi\n",
            "dá\n",
            "da\n",
            "uma\n",
            "umA\n",
            "saudade\n",
            "saudadZ\n",
            "imensa\n",
            "imensA\n",
            "o\n",
            "u\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "jovem\n",
            "zhovEm\n",
            "porque\n",
            "poRki\n",
            "todas\n",
            "todAS\n",
            "as\n",
            "aS\n",
            "férias\n",
            "fehRIAS\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "em\n",
            "em\n",
            "paquetá\n",
            "paketa\n",
            "entao\n",
            "entaU\n",
            "eu\n",
            "eU\n",
            "ia\n",
            "iA\n",
            "daqui\n",
            "daki\n",
            "para\n",
            "paRA\n",
            "o\n",
            "u\n",
            "rio\n",
            "riU\n",
            "de\n",
            "dZi\n",
            "trem\n",
            "tRem\n",
            "e\n",
            "i\n",
            "lá\n",
            "la\n",
            "você\n",
            "vose\n",
            "devia\n",
            "deviA\n",
            "dizer\n",
            "dZizeR\n",
            "no\n",
            "nu\n",
            "tempo\n",
            "tempU\n",
            "que\n",
            "ki\n",
            "era\n",
            "ehRA\n",
            "mais\n",
            "\n",
            "e\n",
            "i\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "as\n",
            "aS\n",
            "conduçoes\n",
            "kondusoIS\n",
            "aqui\n",
            "aki\n",
            "em\n",
            "em\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "por\n",
            "poR\n",
            "exemplo\n",
            "ezemplU\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "mais\n",
            "maIS\n",
            "jovem\n",
            "zhovEm\n",
            "agora\n",
            "agohRA\n",
            "e\n",
            "i\n",
            "e\n",
            "i\n",
            "a\n",
            "a\n",
            "as\n",
            "aS\n",
            "conduçoes\n",
            "kondusoIS\n",
            "aqui\n",
            "aki\n",
            "em\n",
            "em\n",
            "sao\n",
            "saU\n",
            "paulo\n",
            "paUlU\n",
            "por\n",
            "poR\n",
            "exemplo\n",
            "ezemplU\n",
            "aligner: HTK\n",
            "hmmdefs: False\n",
            "aligner: HTK\n",
            "hmmdefs: False\n",
            " -> Loaded 9.4 min of audio. Splitting...\n",
            " -> Segmented into 1537 parts (3.8 min, 0.15 sec avg)\n",
            " -> Wrote 1537 segment wav files\n",
            " -> Progress: 1537 segments, 0.06 hours, 0.15 sec avg\n",
            "Writing metadata for 1537 segments (0.06 hours)\n",
            "Mean: 226.719637\n",
            "Max: 1\n",
            "### Step E\n",
            "[CV 1/1] END p_db_threshold=20.0, p_delta1=0.88, p_delta2=0.7, p_interval_size=3, p_layer=NTB, p_min_words_h2=10, p_segment_name=SP_D2_255, p_segment_number=1, p_silence_threshold=5, p_window_size=0.3;, score=nan total time= 4.3min\n",
            " -> Loaded 9.4 min of audio. Splitting...\n",
            " -> Segmented into 1330 parts (5.6 min, 0.25 sec avg)\n",
            " -> Wrote 1330 segment wav files\n",
            " -> Progress: 1330 segments, 0.09 hours, 0.25 sec avg\n",
            "Writing metadata for 1330 segments (0.09 hours)\n",
            "Mean: 333.717188\n",
            "Max: 1\n",
            "### Step E\n",
            "[CV 1/1] END p_db_threshold=25.0, p_delta1=0.88, p_delta2=0.7, p_interval_size=3, p_layer=NTB, p_min_words_h2=10, p_segment_name=SP_D2_255, p_segment_number=1, p_silence_threshold=5, p_window_size=0.3;, score=nan total time= 4.8min\n",
            "alinhamento feito\n",
            "alinhamento feitoalinhamento feito\n",
            "alinhamento feito\n",
            "alinhamento feito\n",
            "\n",
            "saidas geradas\n",
            "saidas geradas\n",
            "saidas geradassaidas geradas\n",
            "\n",
            "saidas geradas\n",
            "### Step D\n",
            "Loading SP_D2_255_clipped_1: ./08e6bf76742024eda5a4cb86e024d58d/SP_D2_255_clipped_1.wav (1 of 1)\n",
            "### Step D\n",
            "Loading SP_D2_255_clipped_1: ./4c279652036bee31aa8cfb7492c2691f/SP_D2_255_clipped_1.wav (1 of 1)\n",
            "### Step D\n",
            "Loading SP_D2_255_clipped_1: ./0bbc0e07dacc4e52f557a308090cb142/SP_D2_255_clipped_1.wav (1 of 1)\n",
            "### Step D### Step D\n",
            "\n",
            "Loading SP_D2_255_clipped_1: ./4e71449c82c96a1bb626788b6c680793/SP_D2_255_clipped_1.wav (1 of 1)\n",
            "Loading SP_D2_255_clipped_1: ./f3f7053893e6c3d2822cf77e83990902/SP_D2_255_clipped_1.wav (1 of 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-62b4df07c99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'multiprocessing' / 'threading'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMeuSegmentadorBiron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_scoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando os resultados em um CSV\n",
        "Recomendo que o caminho do CSV seja no seu próprio Google Drive."
      ],
      "metadata": {
        "id": "ULiSVguZ6-ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "means = gs.cv_results_['mean_test_score']\n",
        "params = gs.cv_results_['params']\n",
        "\n",
        "L = []\n",
        "for ser, param in zip(means, params):\n",
        "  res = {}\n",
        "  res['ser'] = ser\n",
        "  for p in param:\n",
        "    res[p] = param[p]\n",
        "  L.append(res)\n",
        "df = pd.DataFrame(L)\n",
        "df.to_csv('results.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "MkIukpph271P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# params = {\n",
        "#                  'p_segment_name': ['SP_D2_255'],\n",
        "#                  'p_segment_number': ['1'],\n",
        "#                  'p_db_threshold': [20.0, 25.0, 30.0, 35.0, 40.0],\n",
        "#                  'p_window_size': [0.25, 0.30, 0.35, 0.40, 0.45, 0.50],\n",
        "#                  'p_delta1': [0.6,0.65,0.7,0.75,0.8],\n",
        "#                  'p_delta2': [0.81,0.85,0.91,0.96],\n",
        "#                  'p_interval_size': [0.2, 1.4 , 2.6], \n",
        "#                  'p_silence_threshold': [5.3, 5.6, 5.9],\n",
        "#                  'p_min_words_h2': [4, 9],\n",
        "#                  'p_layer': ['NTB']\n",
        "# }\n",
        "\n",
        "\n",
        "# def custom_cv_g(X,Y):\n",
        "#   v = [0]*len(X)\n",
        "#   yield v, v\n",
        "\n",
        "# #### trick para usar o gridsearch do sklearn\n",
        "# x = ['2','3','4','5','6','7','8']\n",
        "# y = ['0','1','0','1','0','1','0']\n",
        "# custom_cv = custom_cv_g(x,y)\n",
        "# #### trick para usar o gridsearch do sklearn\n",
        "\n",
        "# #### forcando threading\n",
        "# from sklearn.utils import parallel_backend\n",
        "\n",
        "# with parallel_backend('threading'):  # 'multiprocessing' / 'threading'\n",
        "#     gs = GridSearchCV(MeuSegmentadorBiron(), param_grid=params, cv=custom_cv, scoring=my_scoring, refit=False, verbose=3, n_jobs=5)\n",
        "#     gs.fit(x, y)"
      ],
      "metadata": {
        "id": "MIc5T78Z0cln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Versão alternativa com RandomizedSearchCV\n",
        "\n",
        "Útil no caso de faltar tempo ou máquina para os experimentos.\n",
        "Use o parametro n_iter para controlar o número máximo de runs (tempo).\n",
        "\n",
        "Discussão interessante sobre o problema: https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"
      ],
      "metadata": {
        "id": "rAH5vPLFA-_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params = {\n",
        "                 'p_segment_name': ['SP_D2_255'],\n",
        "                 'p_segment_number': ['1'],\n",
        "                 'p_db_threshold': [20.0, 25.0, 30.0, 35.0, 40.0],\n",
        "                 'p_window_size': [0.25, 0.30, 0.35, 0.40, 0.45, 0.50],\n",
        "                 'p_delta1': [0.6,0.65,0.7,0.75,0.8],\n",
        "                 'p_delta2': [0.81,0.85,0.91,0.96],\n",
        "                 'p_interval_size': [0.2, 1.4 , 2.6], \n",
        "                 'p_silence_threshold': [5.3, 5.6, 5.9],\n",
        "                 'p_min_words_h2': [4, 9],\n",
        "                 'p_layer': ['NTB']\n",
        "}\n",
        "\n",
        "def custom_cv_g(X,Y):\n",
        "  v = [0]*len(X)\n",
        "  yield v, v\n",
        "\n",
        "#### trick para usar o gridsearch do sklearn\n",
        "x = ['2','3','4','5','6','7','8']\n",
        "y = ['0','1','0','1','0','1','0']\n",
        "custom_cv = custom_cv_g(x,y)\n",
        "#### trick para usar o gridsearch do sklearn\n",
        "\n",
        "#### forcando mul\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "with parallel_backend('threading'):  # 'multiprocessing' / 'threading'\n",
        "    gs = RandomizedSearchCV(MeuSegmentadorBiron(), n_iter=5, param_distributions=params, cv=custom_cv, scoring=my_scoring, refit=False, verbose=3, n_jobs=5)\n",
        "    gs.fit(x, y)"
      ],
      "metadata": {
        "id": "A0iQ93iFBCJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "means = gs.cv_results_['mean_test_score']\n",
        "params = gs.cv_results_['params']\n",
        "\n",
        "L = []\n",
        "for ser, param in zip(means, params):\n",
        "  res = {}\n",
        "  res['ser'] = ser\n",
        "  for p in param:\n",
        "    res[p] = param[p]\n",
        "  L.append(res)\n",
        "df = pd.DataFrame(L)\n",
        "df.to_csv('results_rs.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "eAAMsU_NCdgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de Parâmetros\n",
        "Exemplos para plot, apenas demonstração. Analisar em outro script."
      ],
      "metadata": {
        "id": "tg4oDldaxYQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # example of grid search for function optimization with plot\n",
        "# from numpy import arange\n",
        "# from numpy import meshgrid\n",
        "# from numpy.random import rand\n",
        "# from matplotlib import pyplot\n",
        "\n",
        "# # objective function\n",
        "# def objective(x, y): # Ajustar aqui para retornar valores capturados pelo gridsearch\n",
        "# \treturn x**2.0 + y**2.0\n",
        "\n",
        "# # define range for input\n",
        "# r_min, r_max = -5.0, 5.0\n",
        "# # generate a grid sample from the domain\n",
        "# sample = list()\n",
        "# step = 0.5\n",
        "# for x in arange(r_min, r_max+step, step):\n",
        "# \tfor y in arange(r_min, r_max+step, step):\n",
        "# \t\tsample.append([x,y])\n",
        "# # evaluate the sample\n",
        "# sample_eval = [objective(x,y) for x,y in sample]\n",
        "# # locate the best solution\n",
        "# best_ix = 0\n",
        "# for i in range(len(sample)):\n",
        "# \tif sample_eval[i] < sample_eval[best_ix]:\n",
        "# \t\tbest_ix = i\n",
        "# # summarize best solution\n",
        "# print(sample)\n",
        "# print('Best: f(%.5f,%.5f) = %.5f' % (sample[best_ix][0], sample[best_ix][1], sample_eval[best_ix]))\n",
        "# # sample input range uniformly at 0.1 increments\n",
        "# xaxis = arange(r_min, r_max, 0.1)\n",
        "# yaxis = arange(r_min, r_max, 0.1)\n",
        "# # create a mesh from the axis\n",
        "# x, y = meshgrid(xaxis, yaxis)\n",
        "# # compute targets\n",
        "# results = objective(x, y)\n",
        "# # create a filled contour plot\n",
        "# pyplot.contourf(x, y, results, levels=50, cmap='jet')\n",
        "# # plot the sample as black circles\n",
        "# pyplot.plot([x for x,_ in sample], [y for _,y in sample], '.', color='black')\n",
        "# # draw the best result as a white star\n",
        "# pyplot.plot(sample[best_ix][0], sample[best_ix][1], '*', color='white')\n",
        "# # show the plot\n",
        "# pyplot.show()"
      ],
      "metadata": {
        "id": "oxbSA-1S80ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import plotly.graph_objects as go\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# fig = go.Figure(data=[go.Surface(z=results, x=x, y=y)])\n",
        "# fig.update_layout(title='Parameter Analysis', autosize=False,\n",
        "#                   width=500, height=500,\n",
        "#                   margin=dict(l=65, r=50, b=65, t=90))\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "HptaAIYRxc9Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}